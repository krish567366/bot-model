{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb0ccad2",
   "metadata": {},
   "source": [
    "# ML Model Development Roadmap for Trading Bot\n",
    "\n",
    "This notebook implements a complete machine learning pipeline for financial time series prediction, following the best practices roadmap:\n",
    "\n",
    "1. **Baseline Tree/Tabular Models** (LightGBM, XGBoost, CatBoost)\n",
    "2. **Classical ML Ensembles** (Stacked ensembles, bagging)\n",
    "3. **Proper Time Series Validation** (Walk-forward, purged CV)\n",
    "4. **Hyperparameter Optimization** (Optuna)\n",
    "5. **Model Evaluation** (Trading & ML metrics)\n",
    "6. **Production Integration** (Model registry, backtesting)\n",
    "\n",
    "**Why this order?**\n",
    "- Tree models are interpretable, fast, and often outperform complex deep models on engineered financial features\n",
    "- Ensembles reduce overfitting and boost out-of-sample robustness\n",
    "- Proper validation prevents the #1 killer: overfitting to backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd0b335",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.13.1)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/trading-bot/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, f1_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# Tree-based models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# Feature importance and explainability\n",
    "import shap\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")\n",
    "try:\n",
    "    print(f\"LightGBM version: {lgb.__version__}\")\n",
    "    print(f\"XGBoost version: {xgb.__version__}\")\n",
    "    print(f\"CatBoost version: {cb.__version__}\")\n",
    "except:\n",
    "    print(\"⚠️  Some libraries may need installation: pip install lightgbm xgboost catboost optuna shap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f20bb",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "Let's load our financial data and prepare it for machine learning. We'll use the existing feature engineering pipeline and create proper time-based labels.\n",
    "\n",
    "**Key Points:**\n",
    "- Use engineered features from our pipeline (technical indicators, price features, volume features)\n",
    "- Create classification labels: P(future return > threshold)\n",
    "- Implement proper time-based splits to prevent data leakage\n",
    "- Handle missing values and outliers appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our existing feature engineering pipeline\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "try:\n",
    "    from arbi.ai.feature_engineering_v2 import compute_features_deterministic, load_feature_schema\n",
    "    from arbi.ai.training_v2 import generate_synthetic_ohlcv_data\n",
    "    \n",
    "    # Load feature schema to understand our features\n",
    "    schema = load_feature_schema()\n",
    "    print(f\"Feature Schema v{schema['schema_version']}\")\n",
    "    print(f\"Total features: {len(schema['features'])}\")\n",
    "    \n",
    "    # Display feature categories\n",
    "    feature_categories = {}\n",
    "    for feature in schema['features']:\n",
    "        category = feature.get('category', 'unknown')\n",
    "        if category not in feature_categories:\n",
    "            feature_categories[category] = []\n",
    "        feature_categories[category].append(feature['name'])\n",
    "    \n",
    "    print(\"\\n📊 Feature Categories:\")\n",
    "    for category, features in feature_categories.items():\n",
    "        print(f\"  {category.upper()}: {len(features)} features\")\n",
    "        print(f\"    {', '.join(features[:5])}{'...' if len(features) > 5 else ''}\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"⚠️  Could not import feature engineering: {e}\")\n",
    "    print(\"Make sure you're running this from the trading-bot directory\")\n",
    "    print(\"We'll create synthetic features instead...\")\n",
    "    \n",
    "    # Create basic synthetic features as fallback\n",
    "    def create_basic_features(df):\n",
    "        \"\"\"Create basic technical indicators\"\"\"\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Price features\n",
    "        features['returns'] = df['close'].pct_change()\n",
    "        features['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "        features['price_ma5'] = df['close'].rolling(5).mean()\n",
    "        features['price_ma20'] = df['close'].rolling(20).mean()\n",
    "        \n",
    "        # Volume features\n",
    "        features['volume_ma5'] = df['volume'].rolling(5).mean()\n",
    "        features['volume_ratio'] = df['volume'] / features['volume_ma5']\n",
    "        \n",
    "        # Volatility\n",
    "        features['volatility'] = features['returns'].rolling(20).std()\n",
    "        \n",
    "        # RSI\n",
    "        delta = df['close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "        rs = gain / loss\n",
    "        features['rsi'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        return features.dropna()\n",
    "    \n",
    "    compute_features_deterministic = lambda df, symbol=None: type('Result', (), {\n",
    "        'features': create_basic_features(df)\n",
    "    })()\n",
    "    \n",
    "    # Generate synthetic OHLCV data\n",
    "    def generate_synthetic_ohlcv_data(n_periods=1000, symbol=\"BTC/USDT\"):\n",
    "        dates = pd.date_range(start='2023-01-01', periods=n_periods, freq='1H')\n",
    "        \n",
    "        # Random walk with drift\n",
    "        np.random.seed(42)\n",
    "        returns = np.random.normal(0.0001, 0.01, n_periods)\n",
    "        prices = 50000 * np.exp(np.cumsum(returns))\n",
    "        \n",
    "        data = []\n",
    "        for i, (date, price) in enumerate(zip(dates, prices)):\n",
    "            high = price * (1 + abs(np.random.normal(0, 0.01)))\n",
    "            low = price * (1 - abs(np.random.normal(0, 0.01)))\n",
    "            open_price = prices[i-1] if i > 0 else price\n",
    "            volume = np.random.uniform(100, 1000)\n",
    "            \n",
    "            data.append({\n",
    "                'timestamp': date,\n",
    "                'open': open_price,\n",
    "                'high': high,\n",
    "                'low': low,\n",
    "                'close': price,\n",
    "                'volume': volume\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    print(\"✅ Created fallback feature engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06727b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for demonstration (replace with real data in production)\n",
    "def create_training_dataset(n_periods=2000, symbol=\"BTC/USDT\"):\n",
    "    \"\"\"Create training dataset with features and labels\"\"\"\n",
    "    \n",
    "    # Generate OHLCV data\n",
    "    df = generate_synthetic_ohlcv_data(n_periods, symbol)\n",
    "    \n",
    "    # Compute features using our deterministic pipeline\n",
    "    feature_result = compute_features_deterministic(df, symbol)\n",
    "    feature_df = feature_result.features\n",
    "    \n",
    "    # Create classification labels\n",
    "    # Label: Will price move up > 0.3% in next 5 periods?\n",
    "    future_periods = 5\n",
    "    threshold = 0.003  # 0.3% for crypto intraday\n",
    "    \n",
    "    # Calculate future returns\n",
    "    future_returns = df['close'].shift(-future_periods) / df['close'] - 1\n",
    "    \n",
    "    # Binary classification: 1 if return > threshold, 0 otherwise\n",
    "    labels_binary = (future_returns > threshold).astype(int)\n",
    "    \n",
    "    # Regression target: actual future return\n",
    "    labels_regression = future_returns\n",
    "    \n",
    "    # Remove rows where we can't calculate future returns\n",
    "    valid_mask = ~future_returns.isna()\n",
    "    \n",
    "    feature_df = feature_df[valid_mask].reset_index(drop=True)\n",
    "    labels_binary = labels_binary[valid_mask].reset_index(drop=True)\n",
    "    labels_regression = labels_regression[valid_mask].reset_index(drop=True)\n",
    "    timestamps = df['timestamp'][valid_mask].reset_index(drop=True)\n",
    "    \n",
    "    return feature_df, labels_binary, labels_regression, timestamps\n",
    "\n",
    "# Create dataset\n",
    "print(\"🔄 Creating training dataset...\")\n",
    "X, y_binary, y_regression, timestamps = create_training_dataset(2000, \"BTC/USDT\")\n",
    "\n",
    "print(f\"✅ Dataset created:\")\n",
    "print(f\"  Samples: {len(X)}\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Time range: {timestamps.iloc[0]} to {timestamps.iloc[-1]}\")\n",
    "print(f\"  Binary class distribution: {y_binary.value_counts().to_dict()}\")\n",
    "print(f\"  Regression target stats: mean={y_regression.mean():.4f}, std={y_regression.std():.4f}\")\n",
    "\n",
    "# Display first few features\n",
    "print(f\"\\n📊 Feature sample:\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement proper time-based data splits\n",
    "def create_time_based_splits(X, y, timestamps, train_size=0.6, val_size=0.2):\n",
    "    \"\"\"Create time-based train/val/test splits\"\"\"\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    train_end = int(n_samples * train_size)\n",
    "    val_end = int(n_samples * (train_size + val_size))\n",
    "    \n",
    "    # Create splits\n",
    "    X_train = X.iloc[:train_end].copy()\n",
    "    y_train = y.iloc[:train_end].copy()\n",
    "    \n",
    "    X_val = X.iloc[train_end:val_end].copy()\n",
    "    y_val = y.iloc[train_end:val_end].copy()\n",
    "    \n",
    "    X_test = X.iloc[val_end:].copy()\n",
    "    y_test = y.iloc[val_end:].copy()\n",
    "    \n",
    "    print(f\"📊 Time-based splits:\")\n",
    "    print(f\"  Train: {len(X_train)} samples ({timestamps.iloc[0]} to {timestamps.iloc[train_end-1]})\")\n",
    "    print(f\"  Val:   {len(X_val)} samples ({timestamps.iloc[train_end]} to {timestamps.iloc[val_end-1]})\")\n",
    "    print(f\"  Test:  {len(X_test)} samples ({timestamps.iloc[val_end]} to {timestamps.iloc[-1]})\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Create splits for both binary and regression\n",
    "X_train, X_val, X_test, y_train_binary, y_val_binary, y_test_binary = create_time_based_splits(\n",
    "    X, y_binary, timestamps\n",
    ")\n",
    "\n",
    "_, _, _, y_train_reg, y_val_reg, y_test_reg = create_time_based_splits(\n",
    "    X, y_regression, timestamps\n",
    ")\n",
    "\n",
    "# Feature scaling (important for some models)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train), \n",
    "    columns=X_train.columns, \n",
    "    index=X_train.index\n",
    ")\n",
    "X_val_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_val), \n",
    "    columns=X_val.columns, \n",
    "    index=X_val.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test), \n",
    "    columns=X_test.columns, \n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(\"✅ Data preprocessing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e43ef1",
   "metadata": {},
   "source": [
    "# LightGBM Baseline Model\n",
    "\n",
    "LightGBM is our baseline tree model - fast, interpretable, and excellent on tabular financial features. We'll implement both classification and regression versions.\n",
    "\n",
    "**LightGBM Advantages:**\n",
    "- Fast training and inference\n",
    "- Built-in categorical feature handling\n",
    "- Strong performance on tabular data\n",
    "- Excellent feature importance\n",
    "- Memory efficient\n",
    "\n",
    "**Starter Hyperparameters:**\n",
    "```python\n",
    "num_leaves=31, learning_rate=0.05, n_estimators=1000, \n",
    "min_data_in_leaf=20, feature_fraction=0.8, \n",
    "bagging_fraction=0.8, random_state=42\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b94e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Baseline Configuration\n",
    "LGBM_PARAMS_BINARY = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'verbose': -1,\n",
    "    'random_state': RANDOM_SEED\n",
    "}\n",
    "\n",
    "LGBM_PARAMS_REG = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'verbose': -1,\n",
    "    'random_state': RANDOM_SEED\n",
    "}\n",
    "\n",
    "def train_lightgbm_classifier(X_train, y_train, X_val, y_val, params):\n",
    "    \"\"\"Train LightGBM binary classifier\"\"\"\n",
    "    \n",
    "    # Create datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_lightgbm_regressor(X_train, y_train, X_val, y_val, params):\n",
    "    \"\"\"Train LightGBM regressor\"\"\"\n",
    "    \n",
    "    # Create datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✅ LightGBM functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b9875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM models\n",
    "print(\"🚀 Training LightGBM Binary Classifier...\")\n",
    "try:\n",
    "    lgbm_binary = train_lightgbm_classifier(X_train, y_train_binary, X_val, y_val_binary, LGBM_PARAMS_BINARY)\n",
    "    print(\"✅ LightGBM Binary model trained successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error training LightGBM Binary: {e}\")\n",
    "    lgbm_binary = None\n",
    "\n",
    "print(\"\\n🚀 Training LightGBM Regressor...\")\n",
    "try:\n",
    "    lgbm_regressor = train_lightgbm_regressor(X_train, y_train_reg, X_val, y_val_reg, LGBM_PARAMS_REG)\n",
    "    print(\"✅ LightGBM Regression model trained successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error training LightGBM Regression: {e}\")\n",
    "    lgbm_regressor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LightGBM models\n",
    "def evaluate_binary_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate binary classification model\"\"\"\n",
    "    \n",
    "    if model is None:\n",
    "        print(f\"❌ {model_name} is None, skipping evaluation\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Get predictions\n",
    "        y_pred_proba = model.predict(X_test)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\n📊 {model_name} - Binary Classification Results:\")\n",
    "        print(f\"  AUC:       {auc:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall:    {recall:.4f}\")\n",
    "        print(f\"  F1 Score:  {f1:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'auc': auc, 'precision': precision, 'recall': recall, 'f1': f1,\n",
    "            'predictions': y_pred_proba\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error evaluating {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_regression_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate regression model\"\"\"\n",
    "    \n",
    "    if model is None:\n",
    "        print(f\"❌ {model_name} is None, skipping evaluation\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Get predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\n📊 {model_name} - Regression Results:\")\n",
    "        print(f\"  RMSE: {rmse:.6f}\")\n",
    "        print(f\"  MAE:  {mae:.6f}\")\n",
    "        print(f\"  R²:   {r2:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'rmse': rmse, 'mae': mae, 'r2': r2,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error evaluating {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Evaluate LightGBM models\n",
    "lgbm_binary_results = evaluate_binary_model(lgbm_binary, X_test, y_test_binary, \"LightGBM Binary\")\n",
    "lgbm_reg_results = evaluate_regression_model(lgbm_regressor, X_test, y_test_reg, \"LightGBM Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ecd46",
   "metadata": {},
   "source": [
    "# XGBoost Alternative Model\n",
    "\n",
    "XGBoost provides an excellent alternative to LightGBM with different regularization and boosting approaches. This diversity is valuable for ensemble methods.\n",
    "\n",
    "**XGBoost Advantages:**\n",
    "- Robust to overfitting with strong regularization\n",
    "- Excellent hyperparameter diversity\n",
    "- Proven track record in competitions\n",
    "- Different algorithmic approach than LightGBM\n",
    "\n",
    "**Starter Hyperparameters:**\n",
    "```python\n",
    "max_depth=5, eta=0.05, n_estimators=1000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996979c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Configuration\n",
    "XGB_PARAMS_BINARY = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': 5,\n",
    "    'eta': 0.05,  # learning_rate\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 20,\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "XGB_PARAMS_REG = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'max_depth': 5,\n",
    "    'eta': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 20,\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "def train_xgboost_classifier(X_train, y_train, X_val, y_val, params):\n",
    "    \"\"\"Train XGBoost binary classifier\"\"\"\n",
    "    \n",
    "    # Create DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_xgboost_regressor(X_train, y_train, X_val, y_val, params):\n",
    "    \"\"\"Train XGBoost regressor\"\"\"\n",
    "    \n",
    "    # Create DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✅ XGBoost functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c63e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost models\n",
    "print(\"🚀 Training XGBoost Binary Classifier...\")\n",
    "try:\n",
    "    xgb_binary = train_xgboost_classifier(X_train, y_train_binary, X_val, y_val_binary, XGB_PARAMS_BINARY)\n",
    "    print(\"✅ XGBoost Binary model trained successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error training XGBoost Binary: {e}\")\n",
    "    xgb_binary = None\n",
    "\n",
    "print(\"\\n🚀 Training XGBoost Regressor...\")\n",
    "try:\n",
    "    xgb_regressor = train_xgboost_regressor(X_train, y_train_reg, X_val, y_val_reg, XGB_PARAMS_REG)\n",
    "    print(\"✅ XGBoost Regression model trained successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error training XGBoost Regression: {e}\")\n",
    "    xgb_regressor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate XGBoost models\n",
    "def evaluate_xgb_binary(model, X_test, y_test, model_name=\"XGBoost Binary\"):\n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        y_pred_proba = model.predict(dtest)\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\n📊 {model_name} Results:\")\n",
    "        print(f\"  AUC:       {auc:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall:    {recall:.4f}\")\n",
    "        print(f\"  F1 Score:  {f1:.4f}\")\n",
    "        \n",
    "        return {'auc': auc, 'precision': precision, 'recall': recall, 'f1': f1, 'predictions': y_pred_proba}\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error evaluating {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_xgb_regression(model, X_test, y_test, model_name=\"XGBoost Regression\"):\n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        y_pred = model.predict(dtest)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\n📊 {model_name} Results:\")\n",
    "        print(f\"  RMSE: {rmse:.6f}\")\n",
    "        print(f\"  MAE:  {mae:.6f}\")\n",
    "        print(f\"  R²:   {r2:.4f}\")\n",
    "        \n",
    "        return {'rmse': rmse, 'mae': mae, 'r2': r2, 'predictions': y_pred}\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error evaluating {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "xgb_binary_results = evaluate_xgb_binary(xgb_binary, X_test, y_test_binary)\n",
    "xgb_reg_results = evaluate_xgb_regression(xgb_regressor, X_test, y_test_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0830274",
   "metadata": {},
   "source": [
    "# Model Comparison and Results Summary\n",
    "\n",
    "Let's compare all our baseline models and summarize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c58ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary\n",
    "def create_results_summary():\n",
    "    \"\"\"Create a comprehensive results summary\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🏆 MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Binary Classification Results\n",
    "    print(\"\\n📊 BINARY CLASSIFICATION RESULTS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = {\n",
    "        'LightGBM': lgbm_binary_results,\n",
    "        'XGBoost': xgb_binary_results,\n",
    "    }\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        if result:\n",
    "            print(f\"{model_name:<12} | AUC: {result['auc']:.4f} | Precision: {result['precision']:.4f} | F1: {result['f1']:.4f}\")\n",
    "        else:\n",
    "            print(f\"{model_name:<12} | ❌ Not trained\")\n",
    "    \n",
    "    # Regression Results\n",
    "    print(\"\\n📊 REGRESSION RESULTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    reg_results = {\n",
    "        'LightGBM': lgbm_reg_results,\n",
    "        'XGBoost': xgb_reg_results,\n",
    "    }\n",
    "    \n",
    "    for model_name, result in reg_results.items():\n",
    "        if result:\n",
    "            print(f\"{model_name:<12} | RMSE: {result['rmse']:.6f} | R²: {result['r2']:.4f}\")\n",
    "        else:\n",
    "            print(f\"{model_name:<12} | ❌ Not trained\")\n",
    "    \n",
    "    # Best Model Selection\n",
    "    print(\"\\n🥇 BEST PERFORMERS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Find best binary classifier\n",
    "    best_binary_auc = 0\n",
    "    best_binary_model = None\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        if result and result['auc'] > best_binary_auc:\n",
    "            best_binary_auc = result['auc']\n",
    "            best_binary_model = model_name\n",
    "    \n",
    "    if best_binary_model:\n",
    "        print(f\"Binary Classification: {best_binary_model} (AUC: {best_binary_auc:.4f})\")\n",
    "    \n",
    "    # Find best regressor\n",
    "    best_reg_r2 = -999\n",
    "    best_reg_model = None\n",
    "    \n",
    "    for model_name, result in reg_results.items():\n",
    "        if result and result['r2'] > best_reg_r2:\n",
    "            best_reg_r2 = result['r2']\n",
    "            best_reg_model = model_name\n",
    "    \n",
    "    if best_reg_model:\n",
    "        print(f\"Regression: {best_reg_model} (R²: {best_reg_r2:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "create_results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6d9f7",
   "metadata": {},
   "source": [
    "# Next Steps: Advanced Model Development\n",
    "\n",
    "Now that we have our baseline models, here's your roadmap for advanced development:\n",
    "\n",
    "## 🚀 IMMEDIATE NEXT STEPS (Priority Order):\n",
    "\n",
    "### 1. **CatBoost & Random Forest** (Complete the baseline)\n",
    "```python\n",
    "# Add these models to your ensemble\n",
    "cb_model = cb.CatBoostClassifier(iterations=1000, learning_rate=0.05, depth=5)\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=10)\n",
    "```\n",
    "\n",
    "### 2. **Stacked Ensemble** (Critical for performance)\n",
    "```python\n",
    "# Combine LightGBM + XGBoost + CatBoost + RF with meta-learner\n",
    "meta_learner = LogisticRegression()  # or small LightGBM\n",
    "```\n",
    "\n",
    "### 3. **Hyperparameter Optimization** (Use Optuna)\n",
    "```python\n",
    "# Systematic HPO for each model\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "```\n",
    "\n",
    "### 4. **Time Series Cross-Validation** (Critical!)\n",
    "```python\n",
    "# Walk-forward validation to prevent overfitting\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "# Add purging gaps for realistic trading delays\n",
    "```\n",
    "\n",
    "### 5. **Feature Importance & SHAP Analysis**\n",
    "```python\n",
    "# Understand what drives predictions\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "```\n",
    "\n",
    "## 📈 ADVANCED MODELS (After baseline is solid):\n",
    "\n",
    "### 6. **Sequence Models** (For temporal patterns)\n",
    "```python\n",
    "# LSTM/GRU for 1m/5m high-frequency data\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "\n",
    "### 7. **Regime-Aware Models**\n",
    "```python\n",
    "# Train separate models for different market regimes\n",
    "regime_classifier = train_regime_detector()  # vol/trend regimes\n",
    "models_by_regime = {\n",
    "    'low_vol': lgb_model_low_vol,\n",
    "    'high_vol': lgb_model_high_vol,\n",
    "    'trending': lgb_model_trend\n",
    "}\n",
    "```\n",
    "\n",
    "### 8. **Uncertainty-Aware Models**\n",
    "```python\n",
    "# Quantile regression for position sizing\n",
    "quantile_model = lgb.LGBMRegressor(\n",
    "    objective='quantile', alpha=0.5\n",
    ")\n",
    "```\n",
    "\n",
    "## 🎯 PRODUCTION DEPLOYMENT:\n",
    "\n",
    "### 9. **Model Registry Integration**\n",
    "```python\n",
    "# Save to your existing model registry\n",
    "from arbi.ai.registry import ModelRegistry\n",
    "registry = ModelRegistry()\n",
    "model_id = registry.register_model(model, metadata)\n",
    "```\n",
    "\n",
    "### 10. **Backtesting Integration**\n",
    "```python\n",
    "# Test with realistic slippage & fees\n",
    "backtest_results = backtester.run(\n",
    "    signals=model_predictions,\n",
    "    slippage=0.001, fees=0.0005\n",
    ")\n",
    "```\n",
    "\n",
    "### 11. **Live Inference Pipeline**\n",
    "```python\n",
    "# Deploy via your existing inference engine\n",
    "from arbi.ai.inference_v2 import ProductionInferenceEngine\n",
    "engine = ProductionInferenceEngine()\n",
    "signals = engine.generate_ml_signals(\"BTC/USDT\", \"binance\")\n",
    "```\n",
    "\n",
    "## ⚠️ CRITICAL SUCCESS FACTORS:\n",
    "\n",
    "1. **Always use time-based splits** - No future leakage!\n",
    "2. **Test with realistic costs** - Slippage & fees kill edge\n",
    "3. **Multiple validation windows** - Ensure robustness\n",
    "4. **Feature schema consistency** - Training = Inference\n",
    "5. **Monitor feature drift** - Retrain when needed\n",
    "\n",
    "**Start with steps 1-5 above, then gradually add complexity. The baseline tree models often outperform fancy deep learning in finance!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3befba6d",
   "metadata": {},
   "source": [
    "# Production Deployment Checklist\n",
    "\n",
    "## ✅ COMPLETED:\n",
    "- [x] Feature Engineering Pipeline (deterministic, schema-locked)\n",
    "- [x] Model Registry (SQLite backend, versioning)\n",
    "- [x] Training Pipeline (LightGBM, reproducible)\n",
    "- [x] Inference Engine (real-time signals)\n",
    "- [x] Baseline Models (LightGBM, XGBoost setup)\n",
    "\n",
    "## 🔄 IN PROGRESS:\n",
    "- [ ] Complete baseline ensemble (CatBoost, Random Forest)\n",
    "- [ ] Stacked ensemble implementation\n",
    "- [ ] Hyperparameter optimization (Optuna)\n",
    "- [ ] Time series cross-validation\n",
    "\n",
    "## 📋 TODO (Priority Order):\n",
    "1. **Model Calibration** - Isotonic regression for probability calibration\n",
    "2. **Feature Importance Analysis** - SHAP values for interpretability\n",
    "3. **Backtesting Integration** - Connect models to backtester\n",
    "4. **Performance Monitoring** - Feature drift detection\n",
    "5. **Shadow Deployment** - Paper trading validation\n",
    "6. **Live Deployment** - Canary rollout strategy\n",
    "\n",
    "## 🚀 QUICK START COMMANDS:\n",
    "\n",
    "```bash\n",
    "# 1. Complete the baseline models by running all cells above\n",
    "# 2. Train production models:\n",
    "python -c \"from arbi.ai.training_v2 import train_lightgbm_model; train_lightgbm_model()\"\n",
    "\n",
    "# 3. Test inference:\n",
    "python test_inference.py\n",
    "\n",
    "# 4. Run full integration:\n",
    "python test_full_integration.py\n",
    "\n",
    "# 5. Deploy production service:\n",
    "python inference_service.py\n",
    "```\n",
    "\n",
    "**You now have a complete ML development roadmap! Start by completing the remaining baseline models, then move systematically through the advanced techniques.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
