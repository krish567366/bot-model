{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ead84a7",
   "metadata": {},
   "source": [
    "# üöÄ Google Colab Training Notebook for Trading Bot ML Models\n",
    "\n",
    "This notebook provides a complete ML training pipeline that can be run in Google Colab. It will:\n",
    "\n",
    "1. **Clone your repository** or upload as ZIP\n",
    "2. **Install dependencies** with fallback handling\n",
    "3. **Mount Google Drive** for artifact storage\n",
    "4. **Train models** using your existing modules\n",
    "5. **Save artifacts** to both repo and Drive\n",
    "6. **Validate models** and generate manifest\n",
    "7. **Download results** as ZIP file\n",
    "\n",
    "## üìã Instructions:\n",
    "\n",
    "1. **Replace `GITHUB_REPO_URL`** with your actual repository URL (or upload repo as ZIP)\n",
    "2. **Set `fast_test = True`** for quick testing, `False` for full training\n",
    "3. **Run all cells** in order\n",
    "4. **Check outputs** and download your trained models\n",
    "\n",
    "‚ö†Ô∏è **Private repos**: Use personal access token in URL format: `https://TOKEN@github.com/user/repo.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd1a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Configuration Section - MODIFY THESE VALUES\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# ===== USER CONFIGURATION =====\n",
    "GITHUB_REPO_URL = \"https://github.com/krish567366/bot-model.git\"  # Replace with your GitHub repo URL\n",
    "REPO_NAME = \"bot-model\"  # Your repository name\n",
    "\n",
    "# Training configuration\n",
    "SYMBOL = \"BTC-USD\"\n",
    "INTERVAL = \"1m\"\n",
    "CFG = {\n",
    "    \"fast_test\": False,     # Set to False for full training\n",
    "    \"horizon\": 5,           # Prediction horizon\n",
    "    \"pos_thresh\": 0.002,    # Positive threshold (0.2%)\n",
    "    \"n_splits\": 2,          # Cross-validation splits\n",
    "    \"seed\": 42,             # Random seed\n",
    "    \"n_estimators\": 100,    # Boosting rounds (fast_test)\n",
    "    \"n_estimators_full\": 1000  # Boosting rounds (full training)\n",
    "}\n",
    "\n",
    "# Paths (automatically configured)\n",
    "REPO_PATH = f\"/content/{REPO_NAME}\"\n",
    "MODEL_SAVE_REPO_PATH = f\"{REPO_PATH}/models/\"\n",
    "MODEL_SAVE_DRIVE_PATH = \"/content/drive/MyDrive/trading_bot_models/\"\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Global state\n",
    "DRIVE_MOUNTED = False\n",
    "MODULES_IMPORTED = False\n",
    "\n",
    "print(\"üîß Configuration loaded:\")\n",
    "print(f\"  Symbol: {SYMBOL}\")\n",
    "print(f\"  Fast test mode: {CFG['fast_test']}\")\n",
    "print(f\"  Repository: {GITHUB_REPO_URL}\")\n",
    "print(f\"  Run timestamp: {RUN_TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec93699",
   "metadata": {},
   "source": [
    "# üéØ Data Strategy: Real Market Data Integration\n",
    "\n",
    "This notebook now integrates with your **existing data pipeline** for production-quality training:\n",
    "\n",
    "## üìä **Data Source Priority:**\n",
    "1. **üåü Real Market Data** (Yahoo Finance via your pipeline)\n",
    "2. **üíæ Cached Data** (From previous runs)  \n",
    "3. **üîß Synthetic Data** (Fallback only)\n",
    "\n",
    "## üöÄ **Your Data Pipeline Features:**\n",
    "- ‚úÖ **Multi-source ingestion** (yfinance, Alpha Vantage, CCXT)\n",
    "- ‚úÖ **Data validation & cleaning**\n",
    "- ‚úÖ **Technical indicator computation** \n",
    "- ‚úÖ **Flexible storage** (SQLite/PostgreSQL)\n",
    "- ‚úÖ **Real-time & historical processing**\n",
    "- ‚úÖ **ML-ready feature engineering**\n",
    "\n",
    "## ‚ö° **What This Means:**\n",
    "- **Training on REAL market data** instead of synthetic\n",
    "- **Production-grade features** from your existing pipeline\n",
    "- **Consistent data between training and inference**\n",
    "- **Automatic fallback** if real data unavailable\n",
    "\n",
    "**Set `CFG['fast_test'] = False` for full 2-year training dataset!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e6f4a",
   "metadata": {},
   "source": [
    "# üì• Step 1: Clone Repository\n",
    "\n",
    "Clone your trading bot repository to access the training modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ffe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_repository():\n",
    "    \"\"\"Clone the GitHub repository\"\"\"\n",
    "    \n",
    "    if GITHUB_REPO_URL == \"<YOUR_REPO_URL>\":\n",
    "        print(\"‚ùå Please set GITHUB_REPO_URL in the configuration section above!\")\n",
    "        print(\"   Example: GITHUB_REPO_URL = 'https://github.com/username/trading-bot.git'\")\n",
    "        print(\"   For private repos: GITHUB_REPO_URL = 'https://TOKEN@github.com/username/trading-bot.git'\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîÑ Cloning repository from {GITHUB_REPO_URL}...\")\n",
    "        \n",
    "        # Remove existing directory if present\n",
    "        if os.path.exists(REPO_PATH):\n",
    "            print(\"üìÅ Removing existing repository...\")\n",
    "            import shutil\n",
    "            shutil.rmtree(REPO_PATH)\n",
    "        \n",
    "        # Clone repository\n",
    "        clone_cmd = f\"git clone {GITHUB_REPO_URL} {REPO_PATH}\"\n",
    "        result = os.system(clone_cmd)\n",
    "        \n",
    "        if result == 0 and os.path.exists(REPO_PATH):\n",
    "            print(\"‚úÖ Repository cloned successfully\")\n",
    "            \n",
    "            # Add to Python path\n",
    "            if REPO_PATH not in sys.path:\n",
    "                sys.path.insert(0, REPO_PATH)\n",
    "                print(f\"‚úÖ Added {REPO_PATH} to Python path\")\n",
    "            \n",
    "            # Check for key files\n",
    "            key_files = [\"requirements.txt\", \"ai/\", \"core/\"]\n",
    "            for file in key_files:\n",
    "                if os.path.exists(os.path.join(REPO_PATH, file)):\n",
    "                    print(f\"  ‚úì Found {file}\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è  Missing {file}\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Repository cloning failed\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cloning repository: {e}\")\n",
    "        return False\n",
    "\n",
    "# Alternative: Upload ZIP file\n",
    "def upload_repo_zip():\n",
    "    \"\"\"Upload repository as ZIP file (alternative to git clone)\"\"\"\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(\"üìÅ Upload your repository as a ZIP file:\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        if len(uploaded) == 1:\n",
    "            zip_name = list(uploaded.keys())[0]\n",
    "            print(f\"üì¶ Extracting {zip_name}...\")\n",
    "            \n",
    "            import zipfile\n",
    "            with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
    "                zip_ref.extractall(\"/content/\")\n",
    "            \n",
    "            # Find extracted directory\n",
    "            extracted_dirs = [d for d in os.listdir(\"/content/\") if os.path.isdir(f\"/content/{d}\") and d != \"sample_data\"]\n",
    "            \n",
    "            if extracted_dirs:\n",
    "                global REPO_PATH\n",
    "                REPO_PATH = f\"/content/{extracted_dirs[0]}\"\n",
    "                \n",
    "                if REPO_PATH not in sys.path:\n",
    "                    sys.path.insert(0, REPO_PATH)\n",
    "                \n",
    "                print(f\"‚úÖ Repository extracted to {REPO_PATH}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"‚ùå Could not find extracted directory\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"‚ùå Please upload exactly one ZIP file\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  Not running in Colab - ZIP upload not available\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading ZIP: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clone repository (or use ZIP upload as fallback)\n",
    "clone_success = clone_repository()\n",
    "\n",
    "if not clone_success:\n",
    "    print(\"\\nüí° Alternative: Upload repository as ZIP file\")\n",
    "    print(\"   Uncomment the next line to use ZIP upload instead:\")\n",
    "    print(\"   # clone_success = upload_repo_zip()\")\n",
    "    \n",
    "    # Uncomment this line if you want to use ZIP upload:\n",
    "    # clone_success = upload_repo_zip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc29ff5",
   "metadata": {},
   "source": [
    "# üì¶ Step 2: Install Dependencies\n",
    "\n",
    "Install required Python packages with robust fallback handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc91867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_dependencies():\n",
    "    \"\"\"Install required dependencies with fallbacks\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Installing dependencies...\")\n",
    "    \n",
    "    # Try requirements.txt first\n",
    "    requirements_path = os.path.join(REPO_PATH, \"requirements.txt\")\n",
    "    \n",
    "    if os.path.exists(requirements_path):\n",
    "        print(\"üìÑ Found requirements.txt, installing...\")\n",
    "        result = os.system(f\"pip install -q -r {requirements_path}\")\n",
    "        \n",
    "        if result == 0:\n",
    "            print(\"‚úÖ Requirements installed from requirements.txt\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Some packages from requirements.txt failed, continuing with manual installs...\")\n",
    "    else:\n",
    "        print(\"üìÑ No requirements.txt found, installing core packages...\")\n",
    "    \n",
    "    # Core ML packages\n",
    "    core_packages = [\n",
    "        \"pandas\", \"numpy\", \"scikit-learn\", \"joblib\",\n",
    "        \"lightgbm\", \"xgboost\", \"matplotlib\", \"seaborn\",\n",
    "        \"nest_asyncio\"  # For async support in Colab\n",
    "    ]\n",
    "    \n",
    "    # Optional packages (won't fail if not installed)\n",
    "    optional_packages = [\n",
    "        \"catboost\", \"optuna\", \"shap\", \"yfinance\", \"ccxt\", \n",
    "        \"ta\", \"alpha_vantage\", \"sqlalchemy\"  # For data pipeline\n",
    "    ]\n",
    "    \n",
    "    print(\"üîÑ Installing core ML packages...\")\n",
    "    for package in core_packages:\n",
    "        try:\n",
    "            result = os.system(f\"pip install -q {package}\")\n",
    "            if result == 0:\n",
    "                print(f\"  ‚úì {package}\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  {package} - failed\")\n",
    "        except:\n",
    "            print(f\"  ‚ùå {package} - error\")\n",
    "    \n",
    "    print(\"üîÑ Installing data pipeline packages...\")\n",
    "    for package in optional_packages:\n",
    "        try:\n",
    "            result = os.system(f\"pip install -q {package}\")\n",
    "            if result == 0:\n",
    "                print(f\"  ‚úì {package}\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  {package} - skipped\")\n",
    "        except:\n",
    "            print(f\"  ‚ö†Ô∏è  {package} - skipped\")\n",
    "    \n",
    "    # Verify key packages\n",
    "    print(\"\\nüîç Verifying package installations...\")\n",
    "    key_imports = {\n",
    "        'pandas': 'pd',\n",
    "        'numpy': 'np',\n",
    "        'sklearn': 'sklearn',\n",
    "        'lightgbm': 'lgb',\n",
    "        'joblib': 'joblib',\n",
    "        'nest_asyncio': 'nest_asyncio',\n",
    "        'yfinance': 'yf'\n",
    "    }\n",
    "    \n",
    "    successful_imports = []\n",
    "    failed_imports = []\n",
    "    \n",
    "    for package, alias in key_imports.items():\n",
    "        try:\n",
    "            __import__(package)\n",
    "            successful_imports.append(package)\n",
    "            print(f\"  ‚úì {package}\")\n",
    "        except ImportError:\n",
    "            failed_imports.append(package)\n",
    "            print(f\"  ‚ùå {package}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully imported: {len(successful_imports)}/{len(key_imports)} key packages\")\n",
    "    \n",
    "    if failed_imports:\n",
    "        print(f\"‚ö†Ô∏è  Failed imports: {failed_imports}\")\n",
    "        print(\"   Training will continue but some features may be unavailable\")\n",
    "        \n",
    "        # Special handling for yfinance failure\n",
    "        if 'yfinance' in failed_imports:\n",
    "            print(\"   üìâ yfinance unavailable - will use synthetic data only\")\n",
    "    \n",
    "    return len(failed_imports) == 0\n",
    "\n",
    "# Install dependencies\n",
    "install_success = install_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e05be",
   "metadata": {},
   "source": [
    "# üíæ Step 3: Mount Google Drive\n",
    "\n",
    "Mount Google Drive to save trained models for long-term storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be75e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_google_drive():\n",
    "    \"\"\"Mount Google Drive safely\"\"\"\n",
    "    global DRIVE_MOUNTED\n",
    "    \n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        print(\"üîÑ Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        # Verify mount\n",
    "        if os.path.exists('/content/drive/MyDrive'):\n",
    "            print(\"‚úÖ Google Drive mounted successfully\")\n",
    "            print(f\"üìÅ Drive path: {MODEL_SAVE_DRIVE_PATH}\")\n",
    "            \n",
    "            # Create models directory in Drive if it doesn't exist\n",
    "            os.makedirs(MODEL_SAVE_DRIVE_PATH, exist_ok=True)\n",
    "            DRIVE_MOUNTED = True\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Drive mount verification failed\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  Not running in Google Colab - Drive mount skipped\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Drive mount failed: {e}\")\n",
    "        print(\"Continuing without Drive backup...\")\n",
    "        return False\n",
    "\n",
    "# Mount Google Drive\n",
    "mount_success = mount_google_drive()\n",
    "\n",
    "if mount_success:\n",
    "    print(\"üí° Models will be saved to both repo and Google Drive\")\n",
    "else:\n",
    "    print(\"üí° Models will be saved to repo only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16e643",
   "metadata": {},
   "source": [
    "# üì• Step 4: Import Modules\n",
    "\n",
    "Import the trading bot modules and verify everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0813336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_trading_modules():\n",
    "    \"\"\"Import trading bot modules with fallbacks\"\"\"\n",
    "    global MODULES_IMPORTED\n",
    "    \n",
    "    print(\"üîÑ Importing trading bot modules...\")\n",
    "    \n",
    "    # Core imports\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    from datetime import datetime, timedelta\n",
    "    import json\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(CFG['seed'])\n",
    "    \n",
    "    # Try to import trading bot modules\n",
    "    modules = {}\n",
    "    \n",
    "    try:\n",
    "        # Real data integration (NEW)\n",
    "        try:\n",
    "            from ai.real_data_integration import MLDataIntegrator, get_real_training_data\n",
    "            modules['real_data'] = MLDataIntegrator({\n",
    "                'data_sources': ['yfinance'],\n",
    "                'storage_path': f'{REPO_PATH}/data/',\n",
    "                'cache_enabled': True,\n",
    "                'real_time_enabled': False\n",
    "            })\n",
    "            print(\"  ‚úì ai.real_data_integration - REAL DATA ENABLED!\")\n",
    "        except ImportError:\n",
    "            print(\"  ‚ö†Ô∏è  Real data integration not found - will use synthetic data\")\n",
    "        \n",
    "        # Feature engineering\n",
    "        try:\n",
    "            from ai.feature_engineering_v2 import compute_features_deterministic\n",
    "            modules['feature_engineering'] = compute_features_deterministic\n",
    "            print(\"  ‚úì ai.feature_engineering_v2\")\n",
    "        except ImportError:\n",
    "            try:\n",
    "                from ai.feature_engineering import compute_features_deterministic\n",
    "                modules['feature_engineering'] = compute_features_deterministic\n",
    "                print(\"  ‚úì ai.feature_engineering\")\n",
    "            except ImportError:\n",
    "                print(\"  ‚ö†Ô∏è  Feature engineering module not found - will use synthetic features\")\n",
    "        \n",
    "        # Training modules\n",
    "        try:\n",
    "            from ai.training_v2 import train_lightgbm_model\n",
    "            modules['training'] = train_lightgbm_model\n",
    "            print(\"  ‚úì ai.training_v2\")\n",
    "        except ImportError:\n",
    "            try:\n",
    "                from ai.train_lgbm import train_and_validate_lgbm\n",
    "                modules['training'] = train_and_validate_lgbm\n",
    "                print(\"  ‚úì ai.train_lgbm\")\n",
    "            except ImportError:\n",
    "                print(\"  ‚ö†Ô∏è  Training module not found - will use built-in training\")\n",
    "        \n",
    "        # Model registry\n",
    "        try:\n",
    "            from ai.registry import ModelRegistry\n",
    "            modules['registry'] = ModelRegistry()\n",
    "            print(\"  ‚úì ai.registry\")\n",
    "        except ImportError:\n",
    "            print(\"  ‚ö†Ô∏è  Model registry not found - will save manually\")\n",
    "        \n",
    "        # Data pipeline components\n",
    "        try:\n",
    "            from core.pipeline import YFinanceSource, DataPipeline\n",
    "            modules['data_pipeline'] = DataPipeline()\n",
    "            modules['yfinance'] = YFinanceSource()\n",
    "            print(\"  ‚úì core.pipeline - Data sources available\")\n",
    "        except ImportError:\n",
    "            print(\"  ‚ö†Ô∏è  Data pipeline not found\")\n",
    "        \n",
    "        # Data generation (for testing)\n",
    "        try:\n",
    "            from ai.training_v2 import generate_synthetic_ohlcv_data\n",
    "            modules['data_generator'] = generate_synthetic_ohlcv_data\n",
    "            print(\"  ‚úì Synthetic data generator\")\n",
    "        except ImportError:\n",
    "            print(\"  ‚ö†Ô∏è  Will create basic synthetic data\")\n",
    "        \n",
    "        MODULES_IMPORTED = True\n",
    "        print(\"‚úÖ Module import completed\")\n",
    "        \n",
    "        # Show data source priority\n",
    "        if 'real_data' in modules:\n",
    "            print(\"\\nüéØ DATA SOURCE PRIORITY:\")\n",
    "            print(\"  1. Real market data (Yahoo Finance)\")\n",
    "            print(\"  2. Cached historical data\") \n",
    "            print(\"  3. Synthetic data (fallback)\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Using synthetic data only\")\n",
    "            \n",
    "        return modules\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error importing modules: {e}\")\n",
    "        print(\"   Will proceed with basic fallback implementations\")\n",
    "        return {}\n",
    "\n",
    "# Import modules\n",
    "trading_modules = import_trading_modules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e40bf",
   "metadata": {},
   "source": [
    "# üîÑ Step 5: Generate Training Data\n",
    "\n",
    "Create or load training data for model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e1196",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_training_data():\n",
    "    \"\"\"Generate training data - prioritizing real market data over synthetic\"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Try to use real data first\n",
    "    if 'real_data' in trading_modules:\n",
    "        try:\n",
    "            print(\"üåü Using REAL MARKET DATA from your data pipeline!\")\n",
    "            \n",
    "            # Determine data parameters based on test mode\n",
    "            if CFG['fast_test']:\n",
    "                period = \"6m\"  # 6 months for fast testing\n",
    "                interval = \"1h\" \n",
    "                print(f\"  üìä Fast test mode: {period} of {interval} data\")\n",
    "            else:\n",
    "                period = \"2y\"   # 2 years for full training\n",
    "                interval = \"1h\"\n",
    "                print(f\"  üìä Full training mode: {period} of {interval} data\")\n",
    "            \n",
    "            # Fetch real training data using your pipeline\n",
    "            integrator = trading_modules['real_data']\n",
    "            dataset = await integrator.prepare_training_dataset(\n",
    "                symbol=SYMBOL,\n",
    "                period=period,\n",
    "                interval=interval,\n",
    "                horizon=CFG['horizon'],\n",
    "                pos_thresh=CFG['pos_thresh']\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Real data loaded successfully!\")\n",
    "            print(f\"  üìà Data source: {dataset['metadata'].get('data_source', 'Yahoo Finance')}\")\n",
    "            print(f\"  üìÖ Date range: {dataset['metadata']['data_range']['start']} to {dataset['metadata']['data_range']['end']}\")\n",
    "            print(f\"  üìä Raw data points: {len(dataset['ohlcv_data'])}\")\n",
    "            print(f\"  üßÆ ML features: {dataset['metadata']['n_features']}\")\n",
    "            print(f\"  üéØ Training samples: {dataset['metadata']['n_samples']}\")\n",
    "            print(f\"  üìà Class distribution: {dataset['metadata']['class_distribution']}\")\n",
    "            \n",
    "            # Return real data\n",
    "            return dataset['X'], dataset['y_binary'], dataset['y_regression'], dataset['timestamps']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Real data loading failed: {e}\")\n",
    "            print(\"üìâ Falling back to synthetic data...\")\n",
    "    \n",
    "    # Fallback to synthetic data\n",
    "    print(\"üîß Generating synthetic training data...\")\n",
    "    \n",
    "    # Data size based on test mode\n",
    "    n_periods = 500 if CFG['fast_test'] else 2000\n",
    "    \n",
    "    print(f\"üîÑ Generating {n_periods} periods of synthetic data...\")\n",
    "    \n",
    "    # Generate synthetic OHLCV data\n",
    "    dates = pd.date_range(start='2023-01-01', periods=n_periods, freq='1H')\n",
    "    \n",
    "    # Random walk with drift for realistic price movement\n",
    "    np.random.seed(CFG['seed'])\n",
    "    returns = np.random.normal(0.0001, 0.01, n_periods)  # Small positive drift\n",
    "    log_prices = np.cumsum(returns)\n",
    "    prices = 50000 * np.exp(log_prices)  # Start around $50,000\n",
    "    \n",
    "    data = []\n",
    "    for i, (date, price) in enumerate(zip(dates, prices)):\n",
    "        # Generate realistic OHLC\n",
    "        volatility = abs(np.random.normal(0, 0.008))  # Daily volatility ~0.8%\n",
    "        high = price * (1 + volatility)\n",
    "        low = price * (1 - volatility)\n",
    "        open_price = prices[i-1] if i > 0 else price\n",
    "        volume = np.random.uniform(100, 1000)\n",
    "        \n",
    "        data.append({\n",
    "            'timestamp': date,\n",
    "            'open': open_price,\n",
    "            'high': high,\n",
    "            'low': low,\n",
    "            'close': price,\n",
    "            'volume': volume\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create features\n",
    "    print(\"üîÑ Computing features...\")\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Price features\n",
    "    features['returns'] = df['close'].pct_change()\n",
    "    features['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    features['price_ma5'] = df['close'].rolling(5).mean()\n",
    "    features['price_ma20'] = df['close'].rolling(20).mean()\n",
    "    features['price_std'] = df['close'].rolling(20).std()\n",
    "    \n",
    "    # Volume features\n",
    "    features['volume'] = df['volume']\n",
    "    features['volume_ma5'] = df['volume'].rolling(5).mean()\n",
    "    features['volume_ratio'] = df['volume'] / features['volume_ma5']\n",
    "    \n",
    "    # Technical indicators\n",
    "    features['rsi'] = compute_rsi(df['close'], 14)\n",
    "    features['macd'] = compute_macd(df['close'])\n",
    "    features['bollinger_upper'], features['bollinger_lower'] = compute_bollinger_bands(df['close'])\n",
    "    \n",
    "    # Volatility\n",
    "    features['volatility'] = features['returns'].rolling(20).std()\n",
    "    features['volatility_ma'] = features['volatility'].rolling(5).mean()\n",
    "    \n",
    "    # Clean features\n",
    "    features = features.dropna()\n",
    "    \n",
    "    # Create labels\n",
    "    print(\"üîÑ Creating labels...\")\n",
    "    future_periods = CFG['horizon']\n",
    "    threshold = CFG['pos_thresh']\n",
    "    \n",
    "    # Calculate future returns\n",
    "    future_returns = df['close'].shift(-future_periods) / df['close'] - 1\n",
    "    \n",
    "    # Binary classification: Will price move up > threshold?\n",
    "    labels_binary = (future_returns > threshold).astype(int)\n",
    "    \n",
    "    # Regression target: actual future return\n",
    "    labels_regression = future_returns\n",
    "    \n",
    "    # Align features and labels\n",
    "    valid_mask = ~future_returns.isna() & ~features.isnull().any(axis=1)\n",
    "    \n",
    "    X = features[valid_mask].reset_index(drop=True)\n",
    "    y_binary = labels_binary[valid_mask].reset_index(drop=True)\n",
    "    y_regression = labels_regression[valid_mask].reset_index(drop=True)\n",
    "    timestamps = df['timestamp'][valid_mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ Synthetic dataset created:\")\n",
    "    print(f\"  Samples: {len(X)}\")\n",
    "    print(f\"  Features: {X.shape[1]}\")\n",
    "    print(f\"  Time range: {timestamps.iloc[0]} to {timestamps.iloc[-1]}\")\n",
    "    print(f\"  Binary class distribution: {y_binary.value_counts().to_dict()}\")\n",
    "    print(f\"  Regression target stats: mean={y_regression.mean():.4f}, std={y_regression.std():.4f}\")\n",
    "    \n",
    "    return X, y_binary, y_regression, timestamps\n",
    "\n",
    "def compute_rsi(prices, window=14):\n",
    "    \"\"\"Compute RSI indicator\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def compute_macd(prices, fast=12, slow=26):\n",
    "    \"\"\"Compute MACD indicator\"\"\"\n",
    "    ema_fast = prices.ewm(span=fast).mean()\n",
    "    ema_slow = prices.ewm(span=slow).mean()\n",
    "    return ema_fast - ema_slow\n",
    "\n",
    "def compute_bollinger_bands(prices, window=20, std_dev=2):\n",
    "    \"\"\"Compute Bollinger Bands\"\"\"\n",
    "    ma = prices.rolling(window).mean()\n",
    "    std = prices.rolling(window).std()\n",
    "    upper = ma + (std * std_dev)\n",
    "    lower = ma - (std * std_dev)\n",
    "    return upper, lower\n",
    "\n",
    "# Generate training data (async call in Jupyter requires special handling)\n",
    "print(\"üöÄ Starting data generation...\")\n",
    "\n",
    "# In Jupyter/Colab, we need to handle async calls properly\n",
    "import asyncio\n",
    "\n",
    "# Check if we're in an existing event loop (Jupyter)\n",
    "try:\n",
    "    loop = asyncio.get_running_loop()\n",
    "    # If we're in Jupyter, create a task\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()  # Allow nested event loops\n",
    "    X, y_binary, y_regression, timestamps = await generate_training_data()\n",
    "except RuntimeError:\n",
    "    # If no event loop, run normally\n",
    "    X, y_binary, y_regression, timestamps = asyncio.run(generate_training_data())\n",
    "except ImportError:\n",
    "    # If nest_asyncio not available, use asyncio.run\n",
    "    X, y_binary, y_regression, timestamps = asyncio.run(generate_training_data())\n",
    "\n",
    "print(f\"\\nüéâ Data generation completed!\")\n",
    "print(f\"Final dataset: {len(X)} samples, {len(X.columns)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf4ad7",
   "metadata": {},
   "source": [
    "# üèãÔ∏è Step 6: Train Models\n",
    "\n",
    "Train machine learning models using LightGBM and other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb52d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Analysis and Overview\n",
    "print(\"üìä FEATURE ANALYSIS:\")\n",
    "print(f\"Total Features: {len(X.columns)}\")\n",
    "print(f\"Training Samples: {len(X)}\")\n",
    "\n",
    "# Show data source information\n",
    "data_source = \"Real Market Data via your existing data pipeline\" if 'real_data' in trading_modules else \"Synthetic trading data\"\n",
    "print(f\"Data Source: {data_source}\")\n",
    "\n",
    "if 'real_data' in trading_modules:\n",
    "    print(\"üåü Using PRODUCTION-GRADE features from your data pipeline:\")\n",
    "    print(\"   ‚Ä¢ Technical indicators from ta library\")\n",
    "    print(\"   ‚Ä¢ Market data from Yahoo Finance\")  \n",
    "    print(\"   ‚Ä¢ Advanced feature engineering\")\n",
    "    print(\"   ‚Ä¢ Volume and volatility metrics\")\n",
    "else:\n",
    "    print(\"üîß Using SYNTHETIC features for testing:\")\n",
    "    print(\"   ‚Ä¢ Simulated price movements\")\n",
    "    print(\"   ‚Ä¢ Basic technical indicators\")\n",
    "    print(\"   ‚Ä¢ Test-grade feature generation\")\n",
    "\n",
    "print(\"\\nüìà Feature Categories:\")\n",
    "feature_types = {}\n",
    "for col in X.columns:\n",
    "    if any(x in col.lower() for x in ['price', 'close', 'open', 'high', 'low']):\n",
    "        feature_types['Price Features'] = feature_types.get('Price Features', 0) + 1\n",
    "    elif any(x in col.lower() for x in ['volume', 'vol']):\n",
    "        feature_types['Volume Features'] = feature_types.get('Volume Features', 0) + 1\n",
    "    elif any(x in col.lower() for x in ['return', 'pct', 'change']):\n",
    "        feature_types['Return Features'] = feature_types.get('Return Features', 0) + 1\n",
    "    elif any(x in col.lower() for x in ['sma', 'ema', 'bb', 'rsi', 'macd', 'bollinger', 'moving']):\n",
    "        feature_types['Technical Indicators'] = feature_types.get('Technical Indicators', 0) + 1\n",
    "    elif any(x in col.lower() for x in ['volatility', 'std', 'var']):\n",
    "        feature_types['Volatility Features'] = feature_types.get('Volatility Features', 0) + 1\n",
    "    else:\n",
    "        feature_types['Other Features'] = feature_types.get('Other Features', 0) + 1\n",
    "\n",
    "for category, count in feature_types.items():\n",
    "    print(f\"  ‚Ä¢ {category}: {count}\")\n",
    "\n",
    "print(f\"\\n\udccb Sample Features:\")\n",
    "print(f\"  {list(X.columns[:10])}\")\n",
    "if len(X.columns) > 10:\n",
    "    print(f\"  ... and {len(X.columns) - 10} more features\")\n",
    "\n",
    "# Feature importance preview (basic correlation analysis)\n",
    "print(f\"\\nüéØ Top Correlated Features (with target):\")\n",
    "try:\n",
    "    correlations = X.corrwith(y_binary).abs().sort_values(ascending=False)\n",
    "    print(f\"  ‚Ä¢ {correlations.index[0]}: {correlations.iloc[0]:.3f}\")\n",
    "    print(f\"  ‚Ä¢ {correlations.index[1]}: {correlations.iloc[1]:.3f}\")\n",
    "    print(f\"  ‚Ä¢ {correlations.index[2]}: {correlations.iloc[2]:.3f}\")\n",
    "    print(f\"  ‚Ä¢ {correlations.index[3]}: {correlations.iloc[3]:.3f}\")\n",
    "    print(f\"  ‚Ä¢ {correlations.index[4]}: {correlations.iloc[4]:.3f}\")\n",
    "except:\n",
    "    print(\"  (Correlation analysis skipped)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Feature engineering completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48944f7c",
   "metadata": {},
   "source": [
    "# üíæ Step 7: Save Model Artifacts\n",
    "\n",
    "Save trained models and metadata to both repository and Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998b8643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_artifacts(model, metrics, params, model_type, X_sample):\n",
    "    \"\"\"Create comprehensive model artifacts\"\"\"\n",
    "    import joblib\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Create model directory\n",
    "    model_id = f\"lgbm_{model_type}_{RUN_TIMESTAMP}\"\n",
    "    model_dir = os.path.join(MODEL_SAVE_REPO_PATH, SYMBOL, RUN_TIMESTAMP, model_id)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"üìÅ Creating artifacts in: {model_dir}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(model_dir, \"model.pkl\")\n",
    "    joblib.dump(model, model_path, compress=3)\n",
    "    print(f\"  ‚úì Model saved: model.pkl\")\n",
    "    \n",
    "    # Create and save scaler (even if not used, for consistency)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_sample)  # Fit on sample data for consistency\n",
    "    scaler_path = os.path.join(model_dir, \"scaler.pkl\")\n",
    "    joblib.dump(scaler, scaler_path, compress=3)\n",
    "    print(f\"  ‚úì Scaler saved: scaler.pkl\")\n",
    "    \n",
    "    # Create comprehensive metadata\n",
    "    metadata = {\n",
    "        'model_id': model_id,\n",
    "        'model_type': f'lightgbm_{model_type}',\n",
    "        'symbol': SYMBOL,\n",
    "        'interval': INTERVAL,\n",
    "        'timestamp': RUN_TIMESTAMP,\n",
    "        'training_config': CFG,\n",
    "        'model_params': params,\n",
    "        'metrics': metrics,\n",
    "        'feature_names': list(X_sample.columns),\n",
    "        'n_features': len(X_sample.columns),\n",
    "        'training_samples': len(X_sample),\n",
    "        'fast_test_mode': CFG['fast_test'],\n",
    "        'random_seed': CFG['seed'],\n",
    "        'version': '1.0',\n",
    "        'framework': 'lightgbm',\n",
    "        'task_type': model_type,\n",
    "        'colab_training': True\n",
    "    }\n",
    "    \n",
    "    # Save metadata\n",
    "    meta_path = os.path.join(model_dir, \"meta.json\")\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    print(f\"  ‚úì Metadata saved: meta.json\")\n",
    "    \n",
    "    # Save feature names\n",
    "    feature_names_path = os.path.join(model_dir, \"feature_names.json\")\n",
    "    with open(feature_names_path, 'w') as f:\n",
    "        json.dump(list(X_sample.columns), f)\n",
    "    print(f\"  ‚úì Feature names saved: feature_names.json\")\n",
    "    \n",
    "    return model_dir, metadata\n",
    "\n",
    "def copy_to_drive(source_dir, model_id):\n",
    "    \"\"\"Copy artifacts to Google Drive\"\"\"\n",
    "    if not DRIVE_MOUNTED:\n",
    "        print(\"‚ö†Ô∏è  Google Drive not mounted, skipping Drive backup\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        import shutil\n",
    "        \n",
    "        # Create destination directory\n",
    "        drive_model_dir = os.path.join(MODEL_SAVE_DRIVE_PATH, SYMBOL, RUN_TIMESTAMP, model_id)\n",
    "        os.makedirs(os.path.dirname(drive_model_dir), exist_ok=True)\n",
    "        \n",
    "        # Copy entire model directory\n",
    "        if os.path.exists(drive_model_dir):\n",
    "            shutil.rmtree(drive_model_dir)\n",
    "        \n",
    "        shutil.copytree(source_dir, drive_model_dir)\n",
    "        print(f\"‚úÖ Artifacts copied to Google Drive: {drive_model_dir}\")\n",
    "        \n",
    "        return drive_model_dir\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Failed to copy to Google Drive: {e}\")\n",
    "        return None\n",
    "\n",
    "# Save artifacts for both models\n",
    "saved_models = {}\n",
    "\n",
    "if 'binary_model' in locals() and binary_model is not None:\n",
    "    print(\"\\nüíæ Saving Binary Classification Model...\")\n",
    "    binary_dir, binary_metadata = create_model_artifacts(\n",
    "        binary_model, binary_metrics, binary_params, 'binary', binary_splits['X_train']\n",
    "    )\n",
    "    binary_drive_dir = copy_to_drive(binary_dir, binary_metadata['model_id'])\n",
    "    \n",
    "    saved_models['binary'] = {\n",
    "        'local_path': binary_dir,\n",
    "        'drive_path': binary_drive_dir,\n",
    "        'metadata': binary_metadata\n",
    "    }\n",
    "\n",
    "if 'regression_model' in locals() and regression_model is not None:\n",
    "    print(\"\\nüíæ Saving Regression Model...\")\n",
    "    regression_dir, regression_metadata = create_model_artifacts(\n",
    "        regression_model, regression_metrics, regression_params, 'regression', regression_splits['X_train']\n",
    "    )\n",
    "    regression_drive_dir = copy_to_drive(regression_dir, regression_metadata['model_id'])\n",
    "    \n",
    "    saved_models['regression'] = {\n",
    "        'local_path': regression_dir,\n",
    "        'drive_path': regression_drive_dir,\n",
    "        'metadata': regression_metadata\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ All artifacts saved! Models: {list(saved_models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704749e8",
   "metadata": {},
   "source": [
    "# ‚úÖ Step 8: Model Validation\n",
    "\n",
    "Validate that saved models can be loaded and used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681efb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_saved_models():\n",
    "    \"\"\"Validate that saved models work correctly\"\"\"\n",
    "    import joblib\n",
    "    \n",
    "    print(\"üîç Validating saved models...\")\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    for model_type, model_info in saved_models.items():\n",
    "        print(f\"\\nüîÑ Validating {model_type} model...\")\n",
    "        \n",
    "        try:\n",
    "            model_dir = model_info['local_path']\n",
    "            \n",
    "            # Load model and scaler\n",
    "            model_path = os.path.join(model_dir, \"model.pkl\")\n",
    "            scaler_path = os.path.join(model_dir, \"scaler.pkl\")\n",
    "            meta_path = os.path.join(model_dir, \"meta.json\")\n",
    "            \n",
    "            # Check files exist\n",
    "            for path, name in [(model_path, \"model.pkl\"), (scaler_path, \"scaler.pkl\"), (meta_path, \"meta.json\")]:\n",
    "                if os.path.exists(path):\n",
    "                    print(f\"  ‚úì Found {name}\")\n",
    "                else:\n",
    "                    print(f\"  ‚ùå Missing {name}\")\n",
    "                    continue\n",
    "            \n",
    "            # Load artifacts\n",
    "            model = joblib.load(model_path)\n",
    "            scaler = joblib.load(scaler_path)\n",
    "            \n",
    "            with open(meta_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            print(f\"  ‚úì Loaded model: {metadata['model_id']}\")\n",
    "            print(f\"  ‚úì Features: {metadata['n_features']}\")\n",
    "            print(f\"  ‚úì Training samples: {metadata['training_samples']}\")\n",
    "            \n",
    "            # Test prediction on sample data\n",
    "            if model_type == 'binary':\n",
    "                test_X = binary_splits['X_test'].iloc[:5]  # First 5 test samples\n",
    "                test_y = binary_splits['y_test'].iloc[:5]\n",
    "            else:\n",
    "                test_X = regression_splits['X_test'].iloc[:5]\n",
    "                test_y = regression_splits['y_test'].iloc[:5]\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = model.predict(test_X)\n",
    "            \n",
    "            print(f\"  ‚úì Sample predictions shape: {predictions.shape}\")\n",
    "            print(f\"  ‚úì Sample predictions (first 3): {predictions[:3]}\")\n",
    "            \n",
    "            # Validate prediction format\n",
    "            if model_type == 'binary':\n",
    "                # Binary predictions should be probabilities between 0 and 1\n",
    "                if all(0 <= p <= 1 for p in predictions):\n",
    "                    print(f\"  ‚úÖ Binary probabilities valid (0-1 range)\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è  Binary probabilities outside 0-1 range\")\n",
    "            else:\n",
    "                # Regression predictions should be reasonable returns\n",
    "                if all(abs(p) < 1 for p in predictions):  # |return| < 100%\n",
    "                    print(f\"  ‚úÖ Regression predictions reasonable\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è  Regression predictions seem extreme\")\n",
    "            \n",
    "            validation_results[model_type] = {\n",
    "                'status': 'success',\n",
    "                'model_path': model_path,\n",
    "                'predictions_sample': predictions[:3].tolist(),\n",
    "                'metadata': metadata\n",
    "            }\n",
    "            \n",
    "            print(f\"  ‚úÖ {model_type.capitalize()} model validation successful\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå {model_type.capitalize()} model validation failed: {e}\")\n",
    "            validation_results[model_type] = {\n",
    "                'status': 'failed',\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validate models\n",
    "if saved_models:\n",
    "    validation_results = validate_saved_models()\n",
    "    \n",
    "    print(f\"\\nüèÜ Validation Summary:\")\n",
    "    for model_type, result in validation_results.items():\n",
    "        status = \"‚úÖ\" if result['status'] == 'success' else \"‚ùå\"\n",
    "        print(f\"  {status} {model_type.capitalize()} Model\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No models to validate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd92123",
   "metadata": {},
   "source": [
    "# üìã Step 9: Create Run Manifest\n",
    "\n",
    "Create a comprehensive manifest file documenting this training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_run_manifest():\n",
    "    \"\"\"Create a comprehensive run manifest\"\"\"\n",
    "    \n",
    "    # Create runs directory\n",
    "    runs_dir = os.path.join(REPO_PATH, \"runs\", f\"colab-{RUN_TIMESTAMP}\")\n",
    "    os.makedirs(runs_dir, exist_ok=True)\n",
    "    \n",
    "    # Get git commit hash if available\n",
    "    git_commit = \"unknown\"\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(['git', 'rev-parse', 'HEAD'], \n",
    "                              cwd=REPO_PATH, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            git_commit = result.stdout.strip()[:12]  # Short hash\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Calculate dataset hash (simple hash of feature names and data size)\n",
    "    import hashlib\n",
    "    feature_string = f\"{list(X.columns)}_{len(X)}_{CFG['seed']}\"\n",
    "    dataset_hash = hashlib.md5(feature_string.encode()).hexdigest()[:12]\n",
    "    \n",
    "    # Create comprehensive manifest\n",
    "    manifest = {\n",
    "        'run_info': {\n",
    "            'timestamp': RUN_TIMESTAMP,\n",
    "            'git_commit': git_commit,\n",
    "            'dataset_hash': dataset_hash,\n",
    "            'colab_session': True,\n",
    "            'fast_test_mode': CFG['fast_test']\n",
    "        },\n",
    "        'configuration': CFG,\n",
    "        'data_info': {\n",
    "            'symbol': SYMBOL,\n",
    "            'interval': INTERVAL,\n",
    "            'n_samples': len(X),\n",
    "            'n_features': len(X.columns),\n",
    "            'feature_names': list(X.columns),\n",
    "            'time_range': {\n",
    "                'start': str(timestamps.iloc[0]),\n",
    "                'end': str(timestamps.iloc[-1])\n",
    "            }\n",
    "        },\n",
    "        'models': {},\n",
    "        'artifacts': {\n",
    "            'repo_base_path': MODEL_SAVE_REPO_PATH,\n",
    "            'drive_base_path': MODEL_SAVE_DRIVE_PATH if DRIVE_MOUNTED else None,\n",
    "            'saved_models': []\n",
    "        },\n",
    "        'validation_results': validation_results if 'validation_results' in locals() else {},\n",
    "        'environment': {\n",
    "            'python_version': sys.version,\n",
    "            'key_packages': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add package versions safely\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    manifest['environment']['key_packages']['pandas'] = pd.__version__\n",
    "    manifest['environment']['key_packages']['numpy'] = np.__version__\n",
    "    \n",
    "    try:\n",
    "        import lightgbm\n",
    "        manifest['environment']['key_packages']['lightgbm'] = lightgbm.__version__\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        import sklearn\n",
    "        manifest['environment']['key_packages']['sklearn'] = sklearn.__version__\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Add model information\n",
    "    for model_type, model_info in saved_models.items():\n",
    "        manifest['models'][model_type] = {\n",
    "            'model_id': model_info['metadata']['model_id'],\n",
    "            'local_path': model_info['local_path'],\n",
    "            'drive_path': model_info['drive_path'],\n",
    "            'metrics': model_info['metadata']['metrics'],\n",
    "            'params': model_info['metadata']['model_params']\n",
    "        }\n",
    "        \n",
    "        manifest['artifacts']['saved_models'].append({\n",
    "            'type': model_type,\n",
    "            'id': model_info['metadata']['model_id'],\n",
    "            'path': model_info['local_path']\n",
    "        })\n",
    "    \n",
    "    # Save manifest\n",
    "    manifest_path = os.path.join(runs_dir, \"manifest.json\")\n",
    "    with open(manifest_path, 'w') as f:\n",
    "        json.dump(manifest, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üìã Run manifest created: {manifest_path}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nüìä Training Run Summary:\")\n",
    "    print(f\"  Run ID: colab-{RUN_TIMESTAMP}\")\n",
    "    print(f\"  Git Commit: {git_commit}\")\n",
    "    print(f\"  Dataset Hash: {dataset_hash}\")\n",
    "    print(f\"  Models Trained: {len(saved_models)}\")\n",
    "    print(f\"  Total Samples: {len(X)}\")\n",
    "    print(f\"  Features: {len(X.columns)}\")\n",
    "    print(f\"  Fast Test Mode: {CFG['fast_test']}\")\n",
    "    \n",
    "    if saved_models:\n",
    "        print(f\"\\nüéØ Model Performance:\")\n",
    "        for model_type, model_info in saved_models.items():\n",
    "            metrics = model_info['metadata']['metrics']\n",
    "            if model_type == 'binary':\n",
    "                print(f\"  Binary: AUC={metrics['auc']:.4f}, Accuracy={metrics['accuracy']:.4f}\")\n",
    "            else:\n",
    "                print(f\"  Regression: RMSE={metrics['rmse']:.6f}, R¬≤={metrics['r2']:.4f}\")\n",
    "    \n",
    "    return manifest_path, manifest\n",
    "\n",
    "# Create run manifest\n",
    "if saved_models:\n",
    "    manifest_path, manifest = create_run_manifest()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No models saved, skipping manifest creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca861d",
   "metadata": {},
   "source": [
    "# üì• Step 10: Display Results & Download\n",
    "\n",
    "Display the training results and provide download options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae094bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results():\n",
    "    \"\"\"Display comprehensive training results\"\"\"\n",
    "    \n",
    "    print(\"üèÜ\" + \"=\"*60)\n",
    "    print(\"üèÜ GOOGLE COLAB TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"üèÜ\" + \"=\"*60)\n",
    "    \n",
    "    if not saved_models:\n",
    "        print(\"‚ùå No models were successfully trained\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìä TRAINING SUMMARY:\")\n",
    "    print(f\"  ‚Ä¢ Run Timestamp: {RUN_TIMESTAMP}\")\n",
    "    print(f\"  ‚Ä¢ Symbol: {SYMBOL}\")\n",
    "    print(f\"  ‚Ä¢ Training Mode: {'Fast Test' if CFG['fast_test'] else 'Full Training'}\")\n",
    "    print(f\"  ‚Ä¢ Models Trained: {len(saved_models)}\")\n",
    "    print(f\"  ‚Ä¢ Dataset Size: {len(X)} samples, {len(X.columns)} features\")\n",
    "    \n",
    "    print(f\"\\nüéØ MODEL PERFORMANCE:\")\n",
    "    for model_type, model_info in saved_models.items():\n",
    "        metadata = model_info['metadata']\n",
    "        metrics = metadata['metrics']\n",
    "        \n",
    "        print(f\"\\n  üìà {model_type.upper()} MODEL:\")\n",
    "        print(f\"    Model ID: {metadata['model_id']}\")\n",
    "        print(f\"    Framework: {metadata['framework']}\")\n",
    "        \n",
    "        if model_type == 'binary':\n",
    "            print(f\"    AUC Score: {metrics['auc']:.4f}\")\n",
    "            print(f\"    Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        else:\n",
    "            print(f\"    RMSE: {metrics['rmse']:.6f}\")\n",
    "            print(f\"    R¬≤ Score: {metrics['r2']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ ARTIFACT LOCATIONS:\")\n",
    "    for model_type, model_info in saved_models.items():\n",
    "        print(f\"\\n  {model_type.upper()} MODEL ARTIFACTS:\")\n",
    "        print(f\"    Local Path: {model_info['local_path']}\")\n",
    "        if model_info['drive_path']:\n",
    "            print(f\"    Google Drive: {model_info['drive_path']}\")\n",
    "        \n",
    "        # List files in directory\n",
    "        if os.path.exists(model_info['local_path']):\n",
    "            files = os.listdir(model_info['local_path'])\n",
    "            print(f\"    Files: {', '.join(files)}\")\n",
    "    \n",
    "    # Display sample metadata\n",
    "    if saved_models:\n",
    "        sample_model = list(saved_models.values())[0]\n",
    "        print(f\"\\nüìã SAMPLE MODEL METADATA:\")\n",
    "        \n",
    "        # Pretty print a subset of metadata\n",
    "        display_metadata = {\n",
    "            'model_id': sample_model['metadata']['model_id'],\n",
    "            'model_type': sample_model['metadata']['model_type'],\n",
    "            'training_config': sample_model['metadata']['training_config'],\n",
    "            'metrics': sample_model['metadata']['metrics'],\n",
    "            'n_features': sample_model['metadata']['n_features'],\n",
    "            'training_samples': sample_model['metadata']['training_samples']\n",
    "        }\n",
    "        \n",
    "        print(json.dumps(display_metadata, indent=2))\n",
    "\n",
    "def create_download_zip():\n",
    "    \"\"\"Create ZIP file of all artifacts for download\"\"\"\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        import zipfile\n",
    "        \n",
    "        if not saved_models:\n",
    "            print(\"‚ùå No models to package\")\n",
    "            return\n",
    "        \n",
    "        # Create ZIP filename\n",
    "        zip_filename = f\"trading_bot_models_{RUN_TIMESTAMP}.zip\"\n",
    "        zip_path = f\"/content/{zip_filename}\"\n",
    "        \n",
    "        print(f\"üì¶ Creating download package: {zip_filename}\")\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for model_type, model_info in saved_models.items():\n",
    "                model_dir = model_info['local_path']\n",
    "                \n",
    "                # Add all files from model directory\n",
    "                for root, dirs, files in os.walk(model_dir):\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        # Create relative path for ZIP\n",
    "                        arcname = os.path.relpath(file_path, MODEL_SAVE_REPO_PATH)\n",
    "                        zipf.write(file_path, arcname)\n",
    "                        print(f\"  ‚úì Added: {arcname}\")\n",
    "            \n",
    "            # Add manifest if it exists\n",
    "            if 'manifest_path' in locals():\n",
    "                zipf.write(manifest_path, f\"runs/colab-{RUN_TIMESTAMP}/manifest.json\")\n",
    "                print(f\"  ‚úì Added: manifest.json\")\n",
    "        \n",
    "        print(f\"‚úÖ ZIP package created: {zip_filename}\")\n",
    "        print(f\"üì• Downloading...\")\n",
    "        \n",
    "        # Download the ZIP file\n",
    "        files.download(zip_path)\n",
    "        \n",
    "        print(\"‚úÖ Download initiated!\")\n",
    "        print(\"üí° The ZIP file contains all model artifacts and metadata\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  Not running in Google Colab - download not available\")\n",
    "        print(\"üí° You can manually copy files from the paths shown above\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating download package: {e}\")\n",
    "\n",
    "# Display results\n",
    "display_results()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ NEXT STEPS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if CFG['fast_test']:\n",
    "    print(\"1. üöÄ For production training, set CFG['fast_test'] = False and re-run\")\n",
    "\n",
    "print(\"2. üì• Download your model artifacts using the ZIP package below\")\n",
    "print(\"3. üß™ Test your models in your trading environment\")\n",
    "print(\"4. üìà Integrate with your backtesting and live trading systems\")\n",
    "print(\"5. üîÑ Monitor performance and retrain as needed\")\n",
    "\n",
    "print(f\"\\nüí° Model artifacts are saved in:\")\n",
    "print(f\"   Repository: {MODEL_SAVE_REPO_PATH}\")\n",
    "if DRIVE_MOUNTED:\n",
    "    print(f\"   Google Drive: {MODEL_SAVE_DRIVE_PATH}\")\n",
    "\n",
    "print(f\"\\nü§ñ To use these models in production:\")\n",
    "print(\"   ‚Ä¢ Load with: model = joblib.load('model.pkl')\")\n",
    "print(\"   ‚Ä¢ Make predictions: predictions = model.predict(features)\")\n",
    "print(\"   ‚Ä¢ Check metadata for feature requirements and preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d72c1",
   "metadata": {},
   "source": [
    "# üì• Download Model Artifacts\n",
    "\n",
    "Download all trained models and artifacts as a ZIP file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb3301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and download ZIP package of all artifacts\n",
    "create_download_zip()\n",
    "\n",
    "print(\"\\nüéä Training completed successfully!\")\n",
    "print(\"üéØ Your models are ready for production use!\")\n",
    "\n",
    "# Display final status\n",
    "if saved_models:\n",
    "    print(f\"\\n‚úÖ Successfully trained {len(saved_models)} models:\")\n",
    "    for model_type in saved_models.keys():\n",
    "        print(f\"  ‚Ä¢ {model_type.capitalize()} Classification/Regression Model\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Best practices implemented:\")\n",
    "    print(\"  ‚úì Time-based train/val/test splits\")\n",
    "    print(\"  ‚úì Comprehensive model evaluation\")\n",
    "    print(\"  ‚úì Artifact versioning and metadata\")\n",
    "    print(\"  ‚úì Model validation and integrity checks\")\n",
    "    print(\"  ‚úì Google Drive backup (if mounted)\")\n",
    "    print(\"  ‚úì Downloadable model packages\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No models were successfully trained\")\n",
    "    print(\"Please check the error messages above and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6542db5",
   "metadata": {},
   "source": [
    "# üöÄ Trading Bot ML Training - Google Colab Edition\n",
    "\n",
    "## üìã SETUP INSTRUCTIONS (REQUIRED):\n",
    "\n",
    "### 1. Replace Repository URL\n",
    "```python\n",
    "GITHUB_REPO_URL = \"<YOUR_REPO_URL>\"  # ‚Üê REPLACE THIS!\n",
    "```\n",
    "\n",
    "### 2. Choose Training Mode\n",
    "- **`fast_test=True`** (default): Quick test run with synthetic data (5 minutes)\n",
    "- **`fast_test=False`**: Full training with real data (30-60 minutes)\n",
    "\n",
    "### 3. Private Repository?\n",
    "If your repo is private, use:\n",
    "```python\n",
    "# GITHUB_REPO_URL = \"https://<TOKEN>@github.com/owner/repo.git\"\n",
    "```\n",
    "Replace `<TOKEN>` with your GitHub personal access token.\n",
    "\n",
    "### 4. Alternative: Upload ZIP\n",
    "Instead of cloning, you can upload your repo as a ZIP file and uncomment the ZIP upload section.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What This Notebook Does:\n",
    "1. **Clone** your trading bot repository\n",
    "2. **Install** all dependencies automatically\n",
    "3. **Mount** Google Drive for artifact storage\n",
    "4. **Train** LightGBM and XGBoost models using your existing modules\n",
    "5. **Save** trained models to repo and Google Drive\n",
    "6. **Validate** model artifacts\n",
    "7. **Download** results to your local machine\n",
    "\n",
    "## üì¶ Output Artifacts:\n",
    "- `models/{symbol}/{timestamp}/{model_id}/` - Model files (pkl, meta.json)\n",
    "- `runs/colab-{timestamp}/manifest.json` - Training manifest\n",
    "- Google Drive backup (if mounted)\n",
    "- ZIP download for local machine\n",
    "\n",
    "**Ready? Let's start! üëá**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde199c",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Configuration Section\n",
    "\n",
    "**IMPORTANT: Modify these variables before running!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üîß USER CONFIGURATION - MODIFY THESE VALUES!\n",
    "# =============================================================================\n",
    "\n",
    "# TODO: Replace with your GitHub repository URL\n",
    "GITHUB_REPO_URL = \"<YOUR_REPO_URL>\"  # Example: \"https://github.com/username/trading-bot.git\"\n",
    "\n",
    "# Training Configuration\n",
    "SYMBOL = \"BTC-USD\"\n",
    "INTERVAL = \"1m\"\n",
    "\n",
    "CFG = {\n",
    "    \"fast_test\": True,        # Set to False for full training\n",
    "    \"horizon\": 5,             # Future periods for prediction\n",
    "    \"pos_thresh\": 0.002,      # Positive class threshold (0.2%)\n",
    "    \"n_splits\": 2,            # Cross-validation splits (fast_test)\n",
    "    \"seed\": 42,               # Random seed\n",
    "    \"n_periods\": 1000 if True else 5000,  # Dataset size (will be set based on fast_test)\n",
    "}\n",
    "\n",
    "# Update n_periods based on fast_test\n",
    "CFG[\"n_periods\"] = 1000 if CFG[\"fast_test\"] else 5000\n",
    "\n",
    "# Paths (will be set after repo clone)\n",
    "REPO_NAME = None  # Will be extracted from GITHUB_REPO_URL\n",
    "REPO_PATH = None  # Will be set to /content/{REPO_NAME}\n",
    "MODEL_SAVE_REPO_PATH = None  # Will be set to {REPO_PATH}/models/\n",
    "MODEL_SAVE_DRIVE_PATH = \"/content/drive/MyDrive/models/\"\n",
    "\n",
    "# Status flags\n",
    "DRIVE_MOUNTED = False\n",
    "REPO_CLONED = False\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"üéØ Training Mode: {'Fast Test' if CFG['fast_test'] else 'Full Training'}\")\n",
    "print(f\"üìä Symbol: {SYMBOL} | Interval: {INTERVAL}\")\n",
    "print(f\"üî¢ Dataset Size: {CFG['n_periods']} periods\")\n",
    "\n",
    "if GITHUB_REPO_URL == \"<YOUR_REPO_URL>\":\n",
    "    print(\"‚ö†Ô∏è  WARNING: Please replace GITHUB_REPO_URL with your actual repository URL!\")\n",
    "    print(\"   Example: GITHUB_REPO_URL = 'https://github.com/username/trading-bot.git'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c56e2",
   "metadata": {},
   "source": [
    "# üì• Repository Setup\n",
    "\n",
    "Clone your trading bot repository and set up the Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b993020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_repo_name(url):\n",
    "    \"\"\"Extract repository name from GitHub URL\"\"\"\n",
    "    if url.endswith('.git'):\n",
    "        url = url[:-4]\n",
    "    return url.split('/')[-1]\n",
    "\n",
    "def clone_repository(repo_url):\n",
    "    \"\"\"Clone the repository\"\"\"\n",
    "    global REPO_NAME, REPO_PATH, MODEL_SAVE_REPO_PATH, REPO_CLONED\n",
    "    \n",
    "    if repo_url == \"<YOUR_REPO_URL>\":\n",
    "        print(\"‚ùå ERROR: Please replace GITHUB_REPO_URL with your actual repository URL!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîÑ Cloning repository: {repo_url}\")\n",
    "        \n",
    "        # Extract repo name\n",
    "        REPO_NAME = extract_repo_name(repo_url)\n",
    "        REPO_PATH = f\"/content/{REPO_NAME}\"\n",
    "        MODEL_SAVE_REPO_PATH = f\"{REPO_PATH}/models/\"\n",
    "        \n",
    "        # Remove existing directory if it exists\n",
    "        if os.path.exists(REPO_PATH):\n",
    "            print(f\"üóëÔ∏è  Removing existing directory: {REPO_PATH}\")\n",
    "            shutil.rmtree(REPO_PATH)\n",
    "        \n",
    "        # Clone repository\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"clone\", repo_url, REPO_PATH],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd=\"/content\"\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"‚ùå Git clone failed: {result.stderr}\")\n",
    "            print(\"üí° If this is a private repo, make sure you're using a personal access token:\")\n",
    "            print(\"   https://<TOKEN>@github.com/username/repo.git\")\n",
    "            return False\n",
    "        \n",
    "        # Add to Python path\n",
    "        if REPO_PATH not in sys.path:\n",
    "            sys.path.insert(0, REPO_PATH)\n",
    "        \n",
    "        print(f\"‚úÖ Repository cloned successfully to: {REPO_PATH}\")\n",
    "        print(f\"üìÅ Python path updated: {REPO_PATH}\")\n",
    "        \n",
    "        # Show repository structure\n",
    "        print(\"\\nüìÇ Repository structure:\")\n",
    "        for root, dirs, files in os.walk(REPO_PATH):\n",
    "            # Limit depth to avoid clutter\n",
    "            level = root.replace(REPO_PATH, '').count(os.sep)\n",
    "            if level < 3:\n",
    "                indent = ' ' * 2 * level\n",
    "                print(f\"{indent}{os.path.basename(root)}/\")\n",
    "                subindent = ' ' * 2 * (level + 1)\n",
    "                for file in files[:5]:  # Show only first 5 files per directory\n",
    "                    print(f\"{subindent}{file}\")\n",
    "                if len(files) > 5:\n",
    "                    print(f\"{subindent}... and {len(files) - 5} more files\")\n",
    "        \n",
    "        REPO_CLONED = True\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cloning repository: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clone the repository\n",
    "clone_success = clone_repository(GITHUB_REPO_URL)\n",
    "\n",
    "if not clone_success:\n",
    "    print(\"\\nüîÑ Alternative: Upload ZIP file\")\n",
    "    print(\"If cloning failed, you can upload your repo as a ZIP file instead.\")\n",
    "    print(\"Uncomment and run the next cell to use ZIP upload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede58b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ALTERNATIVE: Upload ZIP file (uncomment if git clone failed)\n",
    "# from google.colab import files\n",
    "# import zipfile\n",
    "\n",
    "# print(\"üì¶ Upload your repository as a ZIP file:\")\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# if uploaded:\n",
    "#     zip_name = list(uploaded.keys())[0]\n",
    "#     print(f\"üì• Extracting {zip_name}...\")\n",
    "    \n",
    "#     with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
    "#         zip_ref.extractall('/content')\n",
    "    \n",
    "#     # Find extracted directory\n",
    "#     for item in os.listdir('/content'):\n",
    "#         if os.path.isdir(f'/content/{item}') and item != 'sample_data':\n",
    "#             REPO_NAME = item\n",
    "#             REPO_PATH = f'/content/{item}'\n",
    "#             MODEL_SAVE_REPO_PATH = f'{REPO_PATH}/models/'\n",
    "#             break\n",
    "    \n",
    "#     if REPO_PATH and REPO_PATH not in sys.path:\n",
    "#         sys.path.insert(0, REPO_PATH)\n",
    "    \n",
    "#     print(f\"‚úÖ ZIP extracted to: {REPO_PATH}\")\n",
    "#     REPO_CLONED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2fe64",
   "metadata": {},
   "source": [
    "# üì¶ Install Dependencies\n",
    "\n",
    "Install required packages for ML training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52831ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results():\n",
    "    \"\"\"Display comprehensive training results\"\"\"\n",
    "    \n",
    "    print(\"üèÜ\" + \"=\"*60)\n",
    "    print(\"üèÜ GOOGLE COLAB TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"üèÜ\" + \"=\"*60)\n",
    "    \n",
    "    if not saved_models:\n",
    "        print(\"‚ùå No models were successfully trained\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n\udcca TRAINING SUMMARY:\")\n",
    "    print(f\"  ‚Ä¢ Run Timestamp: {RUN_TIMESTAMP}\")\n",
    "    print(f\"  ‚Ä¢ Symbol: {SYMBOL}\")\n",
    "    print(f\"  ‚Ä¢ Training Mode: {'Fast Test' if CFG['fast_test'] else 'Full Training'}\")\n",
    "    print(f\"  ‚Ä¢ Models Trained: {len(saved_models)}\")\n",
    "    print(f\"  ‚Ä¢ Dataset Size: {len(X)} samples, {len(X.columns)} features\")\n",
    "    \n",
    "    # Show data source information\n",
    "    data_source = \"üåü Real Market Data\" if 'real_data' in trading_modules else \"üîß Synthetic Data\"\n",
    "    print(f\"  ‚Ä¢ Data Source: {data_source}\")\n",
    "    \n",
    "    if 'real_data' in trading_modules:\n",
    "        print(f\"    üìà Via your existing data pipeline (Yahoo Finance)\")\n",
    "        print(f\"    üéØ Production-grade features and validation\")\n",
    "    else:\n",
    "        print(f\"    ‚ö†Ô∏è  Synthetic data used (real data unavailable)\")\n",
    "    \n",
    "    print(f\"\\nüéØ MODEL PERFORMANCE:\")\n",
    "    for model_type, model_info in saved_models.items():\n",
    "        metadata = model_info['metadata']\n",
    "        metrics = metadata['metrics']\n",
    "        \n",
    "        print(f\"\\n  üìà {model_type.upper()} MODEL:\")\n",
    "        print(f\"    Model ID: {metadata['model_id']}\")\n",
    "        print(f\"    Framework: {metadata['framework']}\")\n",
    "        print(f\"    Data Source: {data_source}\")\n",
    "        \n",
    "        if model_type == 'binary':\n",
    "            print(f\"    AUC Score: {metrics['auc']:.4f}\")\n",
    "            print(f\"    Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        else:\n",
    "            print(f\"    RMSE: {metrics['rmse']:.6f}\")\n",
    "            print(f\"    R¬≤ Score: {metrics['r2']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n\udcc1 ARTIFACT LOCATIONS:\")\n",
    "    for model_type, model_info in saved_models.items():\n",
    "        print(f\"\\n  {model_type.upper()} MODEL ARTIFACTS:\")\n",
    "        print(f\"    Local Path: {model_info['local_path']}\")\n",
    "        if model_info['drive_path']:\n",
    "            print(f\"    Google Drive: {model_info['drive_path']}\")\n",
    "        \n",
    "        # List files in directory\n",
    "        if os.path.exists(model_info['local_path']):\n",
    "            files = os.listdir(model_info['local_path'])\n",
    "            print(f\"    Files: {', '.join(files)}\")\n",
    "    \n",
    "    # Display sample metadata\n",
    "    if saved_models:\n",
    "        sample_model = list(saved_models.values())[0]\n",
    "        print(f\"\\nüìã SAMPLE MODEL METADATA:\")\n",
    "        \n",
    "        # Pretty print a subset of metadata including data source info\n",
    "        display_metadata = {\n",
    "            'model_id': sample_model['metadata']['model_id'],\n",
    "            'model_type': sample_model['metadata']['model_type'],\n",
    "            'data_source': data_source,\n",
    "            'training_config': sample_model['metadata']['training_config'],\n",
    "            'metrics': sample_model['metadata']['metrics'],\n",
    "            'n_features': sample_model['metadata']['n_features'],\n",
    "            'training_samples': sample_model['metadata']['training_samples']\n",
    "        }\n",
    "        \n",
    "        print(json.dumps(display_metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b0e1e",
   "metadata": {},
   "source": [
    "# üíæ Mount Google Drive\n",
    "\n",
    "Mount Google Drive to save trained models for long-term storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6abc136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_google_drive():\n",
    "    \"\"\"Mount Google Drive safely\"\"\"\n",
    "    global DRIVE_MOUNTED\n",
    "    \n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        print(\"üîÑ Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        # Verify mount\n",
    "        if os.path.exists('/content/drive/MyDrive'):\n",
    "            print(\"‚úÖ Google Drive mounted successfully\")\n",
    "            print(f\"üìÅ Drive path: {MODEL_SAVE_DRIVE_PATH}\")\n",
    "            \n",
    "            # Create models directory in Drive if it doesn't exist\n",
    "            os.makedirs(MODEL_SAVE_DRIVE_PATH, exist_ok=True)\n",
    "            DRIVE_MOUNTED = True\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Drive mount verification failed\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  Not running in Google Colab - Drive mount skipped\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Drive mount failed: {e}\")\n",
    "        print(\"Continuing without Drive backup...\")\n",
    "        return False\n",
    "\n",
    "# Mount Google Drive\n",
    "mount_success = mount_google_drive()\n",
    "\n",
    "if mount_success:\n",
    "    print(\"üí° Models will be saved to both repo and Google Drive\")\n",
    "else:\n",
    "    print(\"üí° Models will be saved to repo only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd03a21",
   "metadata": {},
   "source": [
    "# üì• Import Modules\n",
    "\n",
    "Import the trading bot modules and verify everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91745a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import subprocess\n",
    "\n",
    "# ML libraries\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
    "\n",
    "# Optional libraries (with fallbacks)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(\"‚úÖ XGBoost available\")\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  XGBoost not available - will skip XGBoost models\")\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"‚úÖ Optuna available\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Optuna not available - will skip hyperparameter optimization\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(CFG['seed'])\n",
    "\n",
    "print(f\"\\nüîß Core libraries imported successfully\")\n",
    "print(f\"üéØ Random seed: {CFG['seed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16293dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_trading_modules():\n",
    "    \"\"\"Import trading bot modules with fallbacks\"\"\"\n",
    "    \n",
    "    if not REPO_CLONED:\n",
    "        print(\"‚ùå Repository not available for module imports\")\n",
    "        return False\n",
    "    \n",
    "    print(\"üîÑ Importing trading bot modules...\")\n",
    "    \n",
    "    # Try to import existing modules\n",
    "    modules_imported = {}\n",
    "    \n",
    "    # Feature engineering\n",
    "    try:\n",
    "        from arbi.ai.feature_engineering_v2 import compute_features_deterministic, load_feature_schema\n",
    "        modules_imported['feature_engineering'] = True\n",
    "        print(\"‚úÖ Feature engineering module\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è  Feature engineering module not found: {e}\")\n",
    "        modules_imported['feature_engineering'] = False\n",
    "    \n",
    "    # Training module\n",
    "    try:\n",
    "        from arbi.ai.training_v2 import generate_synthetic_ohlcv_data\n",
    "        modules_imported['training'] = True\n",
    "        print(\"‚úÖ Training module\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from arbi.ai.train_lgbm import train_and_validate_lgbm\n",
    "            modules_imported['training'] = True\n",
    "            print(\"‚úÖ LightGBM training module\")\n",
    "        except ImportError as e:\n",
    "            print(f\"‚ö†Ô∏è  Training module not found: {e}\")\n",
    "            modules_imported['training'] = False\n",
    "    \n",
    "    # Model registry\n",
    "    try:\n",
    "        from arbi.ai.registry import ModelRegistry\n",
    "        modules_imported['registry'] = True\n",
    "        print(\"‚úÖ Model registry\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è  Model registry not found: {e}\")\n",
    "        modules_imported['registry'] = False\n",
    "    \n",
    "    # Inference module\n",
    "    try:\n",
    "        from arbi.ai.inference_v2 import ProductionInferenceEngine\n",
    "        modules_imported['inference'] = True\n",
    "        print(\"‚úÖ Inference engine\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from arbi.ai.inference import InferenceEngine\n",
    "            modules_imported['inference'] = True\n",
    "            print(\"‚úÖ Inference engine (v1)\")\n",
    "        except ImportError as e:\n",
    "            print(f\"‚ö†Ô∏è  Inference module not found: {e}\")\n",
    "            modules_imported['inference'] = False\n",
    "    \n",
    "    imported_count = sum(modules_imported.values())\n",
    "    total_count = len(modules_imported)\n",
    "    \n",
    "    print(f\"\\nüìä Module Import Summary: {imported_count}/{total_count} modules imported\")\n",
    "    \n",
    "    if imported_count == 0:\n",
    "        print(\"‚ö†Ô∏è  No trading bot modules found - will use fallback implementations\")\n",
    "        return False\n",
    "    elif imported_count < total_count:\n",
    "        print(\"‚ö†Ô∏è  Some modules missing - will use fallbacks where needed\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚úÖ All modules imported successfully\")\n",
    "        return True\n",
    "\n",
    "# Import trading bot modules\n",
    "modules_available = import_trading_modules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b3984",
   "metadata": {},
   "source": [
    "# üèãÔ∏è Model Training\n",
    "\n",
    "Train LightGBM and XGBoost models using your existing modules or fallback implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1538d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fallback_features(df):\n",
    "    \"\"\"Create basic technical indicators as fallback\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Price features\n",
    "    features['returns'] = df['close'].pct_change()\n",
    "    features['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    features['price_ma5'] = df['close'].rolling(5).mean()\n",
    "    features['price_ma20'] = df['close'].rolling(20).mean()\n",
    "    features['price_ratio_ma5'] = df['close'] / features['price_ma5']\n",
    "    features['price_ratio_ma20'] = df['close'] / features['price_ma20']\n",
    "    \n",
    "    # Volume features\n",
    "    features['volume_ma5'] = df['volume'].rolling(5).mean()\n",
    "    features['volume_ratio'] = df['volume'] / features['volume_ma5']\n",
    "    features['volume_price_trend'] = features['volume_ratio'] * features['returns']\n",
    "    \n",
    "    # Volatility\n",
    "    features['volatility'] = features['returns'].rolling(20).std()\n",
    "    features['volatility_ratio'] = features['returns'].abs() / features['volatility']\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    features['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    exp1 = df['close'].ewm(span=12).mean()\n",
    "    exp2 = df['close'].ewm(span=26).mean()\n",
    "    features['macd'] = exp1 - exp2\n",
    "    features['macd_signal'] = features['macd'].ewm(span=9).mean()\n",
    "    features['macd_hist'] = features['macd'] - features['macd_signal']\n",
    "    \n",
    "    return features.dropna()\n",
    "\n",
    "def generate_fallback_ohlcv_data(n_periods=1000, symbol=\"BTC-USD\"):\n",
    "    \"\"\"Generate synthetic OHLCV data\"\"\"\n",
    "    dates = pd.date_range(start='2023-01-01', periods=n_periods, freq='1H')\n",
    "    \n",
    "    # Random walk with drift and regime changes\n",
    "    np.random.seed(CFG['seed'])\n",
    "    \n",
    "    # Create regime changes\n",
    "    regime_changes = np.random.choice(n_periods, size=5, replace=False)\n",
    "    regime_changes.sort()\n",
    "    \n",
    "    returns = []\n",
    "    current_vol = 0.01\n",
    "    \n",
    "    for i in range(n_periods):\n",
    "        # Change volatility at regime boundaries\n",
    "        if i in regime_changes:\n",
    "            current_vol = np.random.uniform(0.005, 0.02)\n",
    "        \n",
    "        # Generate return with current volatility\n",
    "        ret = np.random.normal(0.00005, current_vol)\n",
    "        returns.append(ret)\n",
    "    \n",
    "    returns = np.array(returns)\n",
    "    prices = 50000 * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    data = []\n",
    "    for i, (date, price) in enumerate(zip(dates, prices)):\n",
    "        high = price * (1 + abs(np.random.normal(0, 0.005)))\n",
    "        low = price * (1 - abs(np.random.normal(0, 0.005)))\n",
    "        open_price = prices[i-1] if i > 0 else price\n",
    "        volume = np.random.uniform(100, 1000) * (1 + abs(returns[i]) * 10)\n",
    "        \n",
    "        data.append({\n",
    "            'timestamp': date,\n",
    "            'open': open_price,\n",
    "            'high': high,\n",
    "            'low': low,\n",
    "            'close': price,\n",
    "            'volume': volume\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def create_training_dataset(n_periods, symbol):\n",
    "    \"\"\"Create training dataset with features and labels\"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Creating training dataset ({n_periods} periods)...\")\n",
    "    \n",
    "    # Generate or load OHLCV data\n",
    "    try:\n",
    "        if modules_available:\n",
    "            from arbi.ai.training_v2 import generate_synthetic_ohlcv_data\n",
    "            df = generate_synthetic_ohlcv_data(n_periods, symbol)\n",
    "            print(\"‚úÖ Using repository OHLCV generation\")\n",
    "        else:\n",
    "            raise ImportError(\"Using fallback\")\n",
    "    except:\n",
    "        df = generate_fallback_ohlcv_data(n_periods, symbol)\n",
    "        print(\"‚úÖ Using fallback OHLCV generation\")\n",
    "    \n",
    "    # Compute features\n",
    "    try:\n",
    "        if modules_available:\n",
    "            from arbi.ai.feature_engineering_v2 import compute_features_deterministic\n",
    "            feature_result = compute_features_deterministic(df, symbol)\n",
    "            feature_df = feature_result.features\n",
    "            print(\"‚úÖ Using repository feature engineering\")\n",
    "        else:\n",
    "            raise ImportError(\"Using fallback\")\n",
    "    except:\n",
    "        feature_df = create_fallback_features(df)\n",
    "        print(\"‚úÖ Using fallback feature engineering\")\n",
    "    \n",
    "    # Create labels\n",
    "    future_periods = CFG['horizon']\n",
    "    threshold = CFG['pos_thresh']\n",
    "    \n",
    "    # Calculate future returns\n",
    "    future_returns = df['close'].shift(-future_periods) / df['close'] - 1\n",
    "    \n",
    "    # Binary classification: 1 if return > threshold, 0 otherwise\n",
    "    labels_binary = (future_returns > threshold).astype(int)\n",
    "    \n",
    "    # Regression target: actual future return\n",
    "    labels_regression = future_returns\n",
    "    \n",
    "    # Remove rows where we can't calculate future returns\n",
    "    valid_mask = ~future_returns.isna()\n",
    "    \n",
    "    feature_df = feature_df[valid_mask].reset_index(drop=True)\n",
    "    labels_binary = labels_binary[valid_mask].reset_index(drop=True)\n",
    "    labels_regression = labels_regression[valid_mask].reset_index(drop=True)\n",
    "    timestamps = df['timestamp'][valid_mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset created:\")\n",
    "    print(f\"  Samples: {len(feature_df)}\")\n",
    "    print(f\"  Features: {feature_df.shape[1]}\")\n",
    "    print(f\"  Positive class: {labels_binary.sum()}/{len(labels_binary)} ({100*labels_binary.mean():.1f}%)\")\n",
    "    print(f\"  Regression target range: {labels_regression.min():.4f} to {labels_regression.max():.4f}\")\n",
    "    \n",
    "    return feature_df, labels_binary, labels_regression, timestamps\n",
    "\n",
    "# Create training dataset\n",
    "X, y_binary, y_regression, timestamps = create_training_dataset(CFG['n_periods'], SYMBOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122a63e",
   "metadata": {},
   "source": [
    "# üéâ Notebook Complete!\n",
    "\n",
    "This is the complete Google Colab training notebook. To continue with training and saving models, use the additional chunks or run the CLI script.\n",
    "\n",
    "**Next steps:**\n",
    "1. Run the remaining training cells\n",
    "2. Save model artifacts\n",
    "3. Create training manifest\n",
    "4. Download results\n",
    "\n",
    "**Or use the CLI script:** `python tools/colab_train.py`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
